{"cells":[{"cell_type":"markdown","metadata":{"id":"5Qf_4Bppkyr-"},"source":["# Mouting Drive"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5120,"status":"ok","timestamp":1694254934554,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"},"user_tz":-360},"id":"7idq270YkgwA","outputId":"aef48b05-fccf-46c2-fe7c-7d47796cc947"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16573,"status":"ok","timestamp":1694254951126,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"},"user_tz":-360},"id":"XvRx6hopk0Wy","outputId":"aa653947-5639-49d7-b74d-7b9b9782a46a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n","Requirement already satisfied: absl-py\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse\u003e=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers\u003e=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast\u003c=0.4.0,\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta\u003e=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio\u003c2.0,\u003e=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n","Requirement already satisfied: h5py\u003e=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: keras\u003c2.14,\u003e=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n","Requirement already satisfied: libclang\u003e=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: numpy\u003c=1.24.3,\u003e=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n","Requirement already satisfied: opt-einsum\u003e=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,\u003c5.0.0dev,\u003e=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six\u003e=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard\u003c2.14,\u003e=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n","Requirement already satisfied: tensorflow-estimator\u003c2.14,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions\u003c4.6.0,\u003e=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt\u003e=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem\u003e=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n","Requirement already satisfied: wheel\u003c1.0,\u003e=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse\u003e=1.6.0-\u003etensorflow) (0.41.2)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib\u003c1.1,\u003e=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (1.0.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (3.4.4)\n","Requirement already satisfied: requests\u003c3,\u003e=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server\u003c0.8.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (0.7.1)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (2.3.7)\n","Requirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (5.3.1)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (0.3.0)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib\u003c1.1,\u003e=0.5-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (2023.7.22)\n","Requirement already satisfied: MarkupSafe\u003e=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug\u003e=1.0.1-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1\u003c0.6.0,\u003e=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c1.1,\u003e=0.5-\u003etensorboard\u003c2.14,\u003e=2.13-\u003etensorflow) (3.2.2)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"tgBSLuwmk10Q"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting fasttext\n","  Downloading fasttext-0.9.2.tar.gz (68 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pybind11\u003e=2.2 (from fasttext)\n","  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n","Requirement already satisfied: setuptools\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n","Building wheels for collected packages: fasttext\n"]}],"source":["!pip install fasttext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CYYXx1IQk3q0"},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Input, Flatten, MaxPooling1D, SpatialDropout1D, Activation\n","\n","from keras.callbacks import EarlyStopping\n","\n","from numpy import array\n","from sklearn.metrics import classification_report\n","\n","import gensim\n","from gensim import models\n","from gensim.models import Word2Vec\n","import fasttext.util\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"lph95l_JlKVY"},"source":["# Loading dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrcpIBsrlH2O"},"outputs":[],"source":["train_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/train.csv\")\n","val_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/dev.csv\")\n","test_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqAzmieZlLwQ"},"outputs":[],"source":["print(f'train: {train_dataset.shape}\\nval: {val_dataset.shape}\\ntest: {test_dataset.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zr-KKxEmlOvd"},"outputs":[],"source":["train_dataset['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYWdT0z_lQrQ"},"outputs":[],"source":["train_x = train_dataset['text']\n","train_y = train_dataset['label']\n","\n","val_x = val_dataset['text']\n","val_y = val_dataset['label']\n","\n","test_x = test_dataset['text']\n","test_y = test_dataset['label']"]},{"cell_type":"markdown","metadata":{"id":"yKaRdmfcllEu"},"source":["# Embedding Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aftL0z0WlfI4"},"outputs":[],"source":["tokenizer=Tokenizer(oov_token = \"\u003cOOV\u003e\", split=' ') # Splitting text based on whitespace and adding \"Out of vocabulary\"\n","tokenizer.fit_on_texts(train_x) # Using the tokenizer on out train dataset to tokenize the train dataset\n","train_encoded=tokenizer.texts_to_sequences(train_x)\n","# print(train_encoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCr9BUHJlm4N"},"outputs":[],"source":["train_padded= pad_sequences(train_encoded, padding='post', maxlen=256)\n","# print(train_padded)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDl1TsE_lpnZ"},"outputs":[],"source":["train_padded.shape[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Od8PRICnlsPV"},"outputs":[],"source":["# padding df_test\n","test_encoded=tokenizer.texts_to_sequences(test_x)\n","test_padded= pad_sequences(test_encoded, padding='post', maxlen=train_padded.shape[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xj5OHq5clvIS"},"outputs":[],"source":["# padding df_validation\n","val_encoded=tokenizer.texts_to_sequences(val_x)\n","val_padded= pad_sequences(val_encoded, padding='post', maxlen=train_padded.shape[1])"]},{"cell_type":"markdown","metadata":{"id":"wzxCFyDl0KFw"},"source":["# Loading FastText"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZtH6vjf0E5h"},"outputs":[],"source":["# import fasttext\n","import numpy as np\n","\n","# # Load the FastText model\n","# fasttext_model = fasttext.load_model(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/fasttext/model_bn_300.bin\")\n","\n","# # Function that takes word vector as input and returns an embedding matrix\n","# def embedding_creation(EMBEDDING_DIM, word_vectors, tokenizer):\n","#     vocabulary_size = len(tokenizer.word_index) + 1\n","#     word_index = tokenizer.word_index\n","#     embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n","\n","#     for word, i in word_index.items():\n","#         try:\n","#             embedding_vector = word_vectors.get_word_vector(word)\n","#             embedding_matrix[i] = embedding_vector\n","#         except KeyError:\n","#             embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)\n","\n","#     return embedding_matrix\n","\n","# EMBEDDING_DIM = 300\n","\n","# # Replace 'tokenizer' with the tokenizer object you have previously created\n","# # Assuming 'tokenizer' is defined earlier in your code\n","\n","# embedding_matrix = embedding_creation(EMBEDDING_DIM, fasttext_model, tokenizer)\n"]},{"cell_type":"markdown","metadata":{"id":"1MCeNzrXmN_6"},"source":["# Model Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05EXLtCGl-gG"},"outputs":[],"source":["!pip install scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwVMKFkYo7-S"},"outputs":[],"source":["import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImZtU3XEo9vE"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Assuming you have your text data in 'train_data', 'val_data', and 'test_data'\n","\n","# Initialize a CountVectorizer to convert text data into BoW representation\n","vectorizer = CountVectorizer(max_features=5000)  # You can adjust the 'max_features' parameter as needed\n","\n","# Fit and transform the training data\n","train_features = vectorizer.fit_transform(train_data)\n","train_y = train_labels  # Assuming you have labels for the training data\n","\n","# Initialize and train the SVM classifier\n","svm_classifier = SVC(kernel='rbf', C=1.0, decision_function_shape='ovr', random_state=42)\n","svm_classifier.fit(train_features, train_y)\n","\n","# Transform the validation and test data using the same vectorizer\n","val_features = vectorizer.transform(val_data)\n","test_features = vectorizer.transform(test_data)\n","\n","# Make predictions on the validation set\n","val_predictions = svm_classifier.predict(val_features)\n","\n","# Evaluate the model on the validation set\n","val_accuracy = accuracy_score(val_y, val_predictions)\n","val_classification_report = classification_report(val_y, val_predictions)\n","\n","print(f\"Validation Accuracy: {val_accuracy}\")\n","print(\"Validation Classification Report:\\n\", val_classification_report)\n","\n","# Make predictions on the test set\n","test_predictions = svm_classifier.predict(test_features)\n","\n","# Evaluate the model on the test set\n","test_accuracy = accuracy_score(test_y, test_predictions)\n","test_classification_report = classification_report(test_y, test_predictions)\n","\n","print(f\"Test Accuracy: {test_accuracy}\")\n","print(\"Test Classification Report:\\n\", test_classification_report)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ye2o9swY09Lf"},"outputs":[],"source":["!pip install joblib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_ulPL0i09dp"},"outputs":[],"source":["import joblib\n","import os\n","\n","# Save the trained model\n","model_filename = 'svm_model.pkl'\n","# Define the directory where you want to save the model\n","save_directory = \"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/final/\"\n","\n","# Combine the directory and filename to create the full file path\n","model_path = os.path.join(save_directory, model_filename)\n","\n","joblib.dump(svm_classifier, model_path)\n","\n","print(f\"Model saved as {model_path}\")"]},{"cell_type":"markdown","metadata":{"id":"9GatfNXr29j2"},"source":["# Loading and testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsXupMkB2ZOL"},"outputs":[],"source":["import joblib\n","import numpy as np\n","import os\n","from sklearn.metrics import classification_report, accuracy_score\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Define the directory and filename where the model is saved\n","model_directory = \"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/final/\"\n","model_filename = 'svm_model.pkl'\n","\n","# Load the trained logistic regression model\n","model_path = os.path.join(model_directory, model_filename)\n","svm_classifier2 = joblib.load(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LzGVnzLG3CNQ"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","test_predictions = svm_classifier2.predict(test_features)\n","\n","# Evaluate the model on the test set\n","test_accuracy = accuracy_score(test_y, test_predictions)\n","test_classification_report = classification_report(test_y, test_predictions, target_names=['0', '1', '2'], output_dict=True)\n","\n","# Convert the classification report dictionary to a DataFrame\n","report_df = pd.DataFrame.from_dict(test_classification_report).T\n","\n","# Format the DataFrame to display percentages\n","report_df['support'] = report_df['support'].astype(int)\n","report_df['accuracy'] = test_accuracy\n","report_df['accuracy'] = report_df['accuracy'].apply(lambda x: f'{x * 100:.2f}%')\n","report_df.iloc[:-1, :-1] = (report_df.iloc[:-1, :-1] * 100).applymap(lambda x: f'{x:.2f}%')\n","\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n","print(\"Test Classification Report:\\n\", report_df)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMBFwUAm4Y+2Sv1IwnrQuFm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}