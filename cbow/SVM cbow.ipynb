{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPtFIH0vrZaoUURNbA7vOsJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Mouting Drive"],"metadata":{"id":"5Qf_4Bppkyr-"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7idq270YkgwA","executionInfo":{"status":"ok","timestamp":1694445050668,"user_tz":-360,"elapsed":19219,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}},"outputId":"8788359d-f7fb-454d-a01b-d0c7a39f90f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvRx6hopk0Wy","executionInfo":{"status":"ok","timestamp":1694445057735,"user_tz":-360,"elapsed":7070,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}},"outputId":"7da5ca27-9c14-4d7f-cf9a-d9a7df5d08b2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n","Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n"]}]},{"cell_type":"code","source":["# !pip install fasttext"],"metadata":{"id":"tgBSLuwmk10Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Input, Flatten, MaxPooling1D, SpatialDropout1D, Activation\n","\n","from keras.callbacks import EarlyStopping\n","\n","from numpy import array\n","from sklearn.metrics import classification_report\n","\n","import gensim\n","from gensim import models\n","from gensim.models import Word2Vec\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"CYYXx1IQk3q0","executionInfo":{"status":"ok","timestamp":1694445061705,"user_tz":-360,"elapsed":3973,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Loading dataset"],"metadata":{"id":"lph95l_JlKVY"}},{"cell_type":"code","source":["train_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/train.csv\")\n","val_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/dev.csv\")\n","test_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/test.csv\")"],"metadata":{"id":"zrcpIBsrlH2O","executionInfo":{"status":"ok","timestamp":1694445071492,"user_tz":-360,"elapsed":489,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(f'train: {train_dataset.shape}\\nval: {val_dataset.shape}\\ntest: {test_dataset.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eqAzmieZlLwQ","executionInfo":{"status":"ok","timestamp":1694445072536,"user_tz":-360,"elapsed":490,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}},"outputId":"ac7cfe8c-d2a9-4dca-ec0f-ab7e91c6371b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["train: (2700, 2)\n","val: (1330, 2)\n","test: (2016, 2)\n"]}]},{"cell_type":"code","source":["train_dataset['label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zr-KKxEmlOvd","executionInfo":{"status":"ok","timestamp":1694445073012,"user_tz":-360,"elapsed":3,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}},"outputId":"31554adc-9c39-4b0c-b445-1a1fed4ee78f"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    1389\n","1     922\n","2     389\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["train_x = train_dataset['text']\n","train_y = train_dataset['label']\n","\n","val_x = val_dataset['text']\n","val_y = val_dataset['label']\n","\n","test_x = test_dataset['text']\n","test_y = test_dataset['label']"],"metadata":{"id":"MYWdT0z_lQrQ","executionInfo":{"status":"ok","timestamp":1694445073012,"user_tz":-360,"elapsed":2,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Embedding Setup"],"metadata":{"id":"yKaRdmfcllEu"}},{"cell_type":"code","source":["tokenizer=Tokenizer(oov_token = \"<OOV>\", split=' ') # Splitting text based on whitespace and adding \"Out of vocabulary\"\n","tokenizer.fit_on_texts(train_x) # Using the tokenizer on out train dataset to tokenize the train dataset\n","train_encoded=tokenizer.texts_to_sequences(train_x)\n","# print(train_encoded)"],"metadata":{"id":"aftL0z0WlfI4","executionInfo":{"status":"ok","timestamp":1694445076805,"user_tz":-360,"elapsed":951,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["train_padded= pad_sequences(train_encoded, padding='post', maxlen=256)\n","# print(train_padded)"],"metadata":{"id":"UCr9BUHJlm4N","executionInfo":{"status":"ok","timestamp":1694445077136,"user_tz":-360,"elapsed":1,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["train_padded.shape[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xDl1TsE_lpnZ","executionInfo":{"status":"ok","timestamp":1694445077981,"user_tz":-360,"elapsed":3,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}},"outputId":"345ad5c8-3158-4a99-92ff-97c50fe66049"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["256"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# padding df_test\n","test_encoded=tokenizer.texts_to_sequences(test_x)\n","test_padded= pad_sequences(test_encoded, padding='post', maxlen=train_padded.shape[1])"],"metadata":{"id":"Od8PRICnlsPV","executionInfo":{"status":"ok","timestamp":1694445079999,"user_tz":-360,"elapsed":541,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# padding df_validation\n","val_encoded=tokenizer.texts_to_sequences(val_x)\n","val_padded= pad_sequences(val_encoded, padding='post', maxlen=train_padded.shape[1])"],"metadata":{"id":"Xj5OHq5clvIS","executionInfo":{"status":"ok","timestamp":1694445080450,"user_tz":-360,"elapsed":2,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Loading FastText"],"metadata":{"id":"wzxCFyDl0KFw"}},{"cell_type":"code","source":["# import fasttext\n","import numpy as np\n","\n","# # Load the FastText model\n","# fasttext_model = fasttext.load_model(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/fasttext/model_bn_300.bin\")\n","\n","# # Function that takes word vector as input and returns an embedding matrix\n","# def embedding_creation(EMBEDDING_DIM, word_vectors, tokenizer):\n","#     vocabulary_size = len(tokenizer.word_index) + 1\n","#     word_index = tokenizer.word_index\n","#     embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n","\n","#     for word, i in word_index.items():\n","#         try:\n","#             embedding_vector = word_vectors.get_word_vector(word)\n","#             embedding_matrix[i] = embedding_vector\n","#         except KeyError:\n","#             embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)\n","\n","#     return embedding_matrix\n","\n","# EMBEDDING_DIM = 300\n","\n","# # Replace 'tokenizer' with the tokenizer object you have previously created\n","# # Assuming 'tokenizer' is defined earlier in your code\n","\n","# embedding_matrix = embedding_creation(EMBEDDING_DIM, fasttext_model, tokenizer)\n"],"metadata":{"id":"7ZtH6vjf0E5h","executionInfo":{"status":"ok","timestamp":1694445082463,"user_tz":-360,"elapsed":2,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Model Configuration"],"metadata":{"id":"1MCeNzrXmN_6"}},{"cell_type":"code","source":["!pip install scikit-learn"],"metadata":{"id":"05EXLtCGl-gG","executionInfo":{"status":"aborted","timestamp":1694445065402,"user_tz":-360,"elapsed":9,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report"],"metadata":{"id":"UwVMKFkYo7-S","executionInfo":{"status":"ok","timestamp":1694445084973,"user_tz":-360,"elapsed":2,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# # Initialize and train the SVM classifier\n","# svm_classifier = SVC(kernel='rbf', C=1.0, decision_function_shape='ovr', random_state=42)\n","# svm_classifier.fit(train_padded, train_y)  # Assuming 'train_y' contains the labels\n","\n","# # Make predictions on the validation set\n","# val_predictions = svm_classifier.predict(val_padded)\n","\n","# # Evaluate the model on the validation set\n","# val_accuracy = accuracy_score(val_y, val_predictions)\n","# val_classification_report = classification_report(val_y, val_predictions)\n","\n","# print(f\"Validation Accuracy: {val_accuracy}\")\n","# print(\"Validation Classification Report:\\n\", val_classification_report)\n","\n","# # Make predictions on the test set\n","# test_predictions = svm_classifier.predict(test_padded)\n","\n","# # Evaluate the model on the test set\n","# test_accuracy = accuracy_score(test_y, test_predictions)\n","# test_classification_report = classification_report(test_y, test_predictions)\n","\n","# print(f\"Test Accuracy: {test_accuracy}\")\n","# print(\"Test Classification Report:\\n\", test_classification_report)\n"],"metadata":{"id":"ImZtU3XEo9vE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from gensim.models import Word2Vec\n","from sklearn import svm\n","from sklearn.metrics import accuracy_score, classification_report\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Load the pre-trained CBOW Word2Vec model\n","cbow_model = Word2Vec.load(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/final/nafi/cbow_model\")\n","\n","# Assuming you have datasets with text and labels: train_dataset, val_dataset, test_dataset\n","# Replace 'texts' and 'labels' with your data\n","train_texts = train_dataset['text'].tolist()\n","val_texts = val_dataset['text'].tolist()\n","test_texts = test_dataset['text'].tolist()\n","\n","train_labels = train_dataset['label'].tolist()\n","val_labels = val_dataset['label'].tolist()\n","test_labels = test_dataset['label'].tolist()\n","\n","# Tokenize the text data\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_texts + val_texts + test_texts)\n","\n","train_sequences = tokenizer.texts_to_sequences(train_texts)\n","val_sequences = tokenizer.texts_to_sequences(val_texts)\n","test_sequences = tokenizer.texts_to_sequences(test_texts)\n","\n","# Pad the sequences to a fixed length\n","max_len = 256  # Set your desired maximum sequence length\n","train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n","val_sequences = pad_sequences(val_sequences, maxlen=max_len, padding='post', truncating='post')\n","test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n","\n","# Create arrays to hold the word embeddings for each sequence\n","embedding_dim = cbow_model.vector_size\n","\n","def get_word_embeddings(sequences):\n","    word_embeddings = []\n","    for seq in sequences:\n","        seq_embeddings = []\n","        for token_idx in seq:\n","            word = tokenizer.index_word.get(token_idx, '')\n","            if word in cbow_model.wv:\n","                word_vector = cbow_model.wv[word]\n","            else:\n","                # Handle out-of-vocabulary words\n","                word_vector = np.zeros(embedding_dim)  # Replace with appropriate handling\n","            seq_embeddings.append(word_vector)\n","        word_embeddings.append(seq_embeddings)\n","    return np.array(word_embeddings)\n","\n","# Convert sequences to word embeddings\n","X_train = get_word_embeddings(train_sequences)\n","X_val = get_word_embeddings(val_sequences)\n","X_test = get_word_embeddings(test_sequences)\n","\n","y_train = np.array(train_labels)\n","y_val = np.array(val_labels)\n","y_test = np.array(test_labels)\n","\n","# Initialize and train the SVM classifier on the training data\n","svm_classifier = svm.SVC(kernel='rbf')\n","svm_classifier.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n","\n","# Make predictions on the validation set\n","val_predictions = svm_classifier.predict(X_val.reshape(X_val.shape[0], -1))\n","\n","# Evaluate the model on the validation set\n","val_accuracy = accuracy_score(y_val, val_predictions)\n","val_classification_report = classification_report(y_val, val_predictions)\n","\n","print(f\"Validation Accuracy: {val_accuracy}\")\n","print(\"Validation Classification Report:\\n\", val_classification_report)\n","\n","# Make predictions on the test set\n","test_predictions = svm_classifier.predict(X_test.reshape(X_test.shape[0], -1))\n","\n","# Evaluate the model on the test set\n","test_accuracy = accuracy_score(y_test, test_predictions)\n","test_classification_report = classification_report(y_test, test_predictions)\n","\n","print(f\"Test Accuracy: {test_accuracy}\")\n","print(\"Test Classification Report:\\n\", test_classification_report)\n"],"metadata":{"id":"AuICrRrZ_Pkb","executionInfo":{"status":"aborted","timestamp":1694445065403,"user_tz":-360,"elapsed":9,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install joblib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ye2o9swY09Lf","executionInfo":{"status":"ok","timestamp":1694434439359,"user_tz":-360,"elapsed":4176,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}},"outputId":"96452381-ef04-481d-b615-be388ecba993"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n"]}]},{"cell_type":"code","source":["import joblib\n","import os\n","\n","# Save the trained model\n","model_filename = 'no_embedding_svm_model.pkl'\n","# Define the directory where you want to save the model\n","save_directory =\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/final/nafi/\"\n","# Combine the directory and filename to create the full file path\n","model_path = os.path.join(save_directory, model_filename)\n","\n","joblib.dump(svm_classifier, model_path)\n","\n","print(f\"Model saved as {model_path}\")"],"metadata":{"id":"Z_ulPL0i09dp","executionInfo":{"status":"ok","timestamp":1694434444149,"user_tz":-360,"elapsed":4792,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"113517eb-c2c8-4fbc-afa7-4c09c71b172c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved as /content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/final/nafi/no_embedding_svm_model.pkl\n"]}]},{"cell_type":"markdown","source":["# Loading and testing"],"metadata":{"id":"9GatfNXr29j2"}},{"cell_type":"code","source":["import joblib\n","import numpy as np\n","import os\n","from sklearn.metrics import classification_report, accuracy_score\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Define the directory and filename where the model is saved\n","model_directory = \"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/final/nafi/\"\n","model_filename = 'no_embedding_svm_model.pkl'\n","\n","# Load the trained logistic regression model\n","model_path = os.path.join(model_directory, model_filename)\n","svm_classifier2 = joblib.load(model_path)"],"metadata":{"id":"rsXupMkB2ZOL","executionInfo":{"status":"ok","timestamp":1694445122892,"user_tz":-360,"elapsed":19806,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","test_predictions = svm_classifier2.predict(X_test.reshape(X_test.shape[0], -1))\n","\n","\n","# Evaluate the model on the test set\n","test_accuracy = accuracy_score(test_y, test_predictions)\n","test_classification_report = classification_report(test_y, test_predictions, target_names=['0', '1', '2'], output_dict=True)\n","\n","# Convert the classification report dictionary to a DataFrame\n","report_df = pd.DataFrame.from_dict(test_classification_report).T\n","\n","# Format the DataFrame to display percentages\n","report_df['support'] = report_df['support'].astype(int)\n","report_df['accuracy'] = test_accuracy\n","report_df['accuracy'] = report_df['accuracy'].apply(lambda x: f'{x * 100:.2f}%')\n","report_df.iloc[:-1, :-1] = (report_df.iloc[:-1, :-1] * 100).applymap(lambda x: f'{x:.2f}%')\n","\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n","print(\"Test Classification Report:\\n\", report_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LzGVnzLG3CNQ","executionInfo":{"status":"ok","timestamp":1694435100624,"user_tz":-360,"elapsed":652102,"user":{"displayName":"Md. Tariquzzaman 190041101","userId":"06887083863639316677"}},"outputId":"f5d04d64-ec0a-4fd5-82d2-d9a77bff8e0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 53.97%\n","Test Classification Report:\n","              precision    recall f1-score     support accuracy\n","0               54.51%    96.99%   69.80%  109600.00%   53.97%\n","1               40.35%     3.20%    5.93%   71900.00%   53.97%\n","2               22.22%     1.00%    1.90%   20100.00%   53.97%\n","accuracy        53.97%    53.97%   53.97%       0.00%   53.97%\n","macro avg       39.03%    33.73%   25.88%  201600.00%   53.97%\n","weighted avg  0.462426  0.539683  0.40249        2016   53.97%\n"]}]}]}