{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tariquzzaman-faisal/VITD/blob/main/LR_cbow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mouting Drive"
      ],
      "metadata": {
        "id": "5Qf_4Bppkyr-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7idq270YkgwA",
        "outputId": "4960b486-c086-4333-bd2f-85afc6f50fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvRx6hopk0Wy",
        "outputId": "d4999da8-76e9-4f33-e313-ea082d51d7a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fasttext"
      ],
      "metadata": {
        "id": "tgBSLuwmk10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Input, Flatten, MaxPooling1D, SpatialDropout1D, Activation\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import gensim\n",
        "from gensim import models\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CYYXx1IQk3q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset"
      ],
      "metadata": {
        "id": "lph95l_JlKVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/train.csv\")\n",
        "val_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/dev.csv\")\n",
        "test_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/test.csv\")"
      ],
      "metadata": {
        "id": "zrcpIBsrlH2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train: {train_dataset.shape}\\nval: {val_dataset.shape}\\ntest: {test_dataset.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqAzmieZlLwQ",
        "outputId": "15c555a7-3d3b-432a-beee-033bcb5c8be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: (2700, 2)\n",
            "val: (1330, 2)\n",
            "test: (2016, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr-KKxEmlOvd",
        "outputId": "c16e21fe-99c1-4690-bfac-056e9d0d70fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1389\n",
              "1     922\n",
              "2     389\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_dataset['text']\n",
        "train_y = train_dataset['label']\n",
        "\n",
        "val_x = val_dataset['text']\n",
        "val_y = val_dataset['label']\n",
        "\n",
        "test_x = test_dataset['text']\n",
        "test_y = test_dataset['label']"
      ],
      "metadata": {
        "id": "MYWdT0z_lQrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Setup"
      ],
      "metadata": {
        "id": "yKaRdmfcllEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(oov_token = \"<OOV>\", split=' ') # Splitting text based on whitespace and adding \"Out of vocabulary\"\n",
        "tokenizer.fit_on_texts(train_x) # Using the tokenizer on out train dataset to tokenize the train dataset\n",
        "train_encoded=tokenizer.texts_to_sequences(train_x)\n",
        "# print(train_encoded)"
      ],
      "metadata": {
        "id": "aftL0z0WlfI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded= pad_sequences(train_encoded, padding='post', maxlen=256)\n",
        "# print(train_padded)"
      ],
      "metadata": {
        "id": "UCr9BUHJlm4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDl1TsE_lpnZ",
        "outputId": "8eb08338-093c-43c0-9bbd-190171c8c0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# padding df_test\n",
        "test_encoded=tokenizer.texts_to_sequences(test_x)\n",
        "test_padded= pad_sequences(test_encoded, padding='post', maxlen=train_padded.shape[1])"
      ],
      "metadata": {
        "id": "Od8PRICnlsPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding df_validation\n",
        "val_encoded=tokenizer.texts_to_sequences(val_x)\n",
        "val_padded= pad_sequences(val_encoded, padding='post', maxlen=train_padded.shape[1])"
      ],
      "metadata": {
        "id": "Xj5OHq5clvIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading FastText"
      ],
      "metadata": {
        "id": "wzxCFyDl0KFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import fasttext\n",
        "import numpy as np\n",
        "\n",
        "# # Load the FastText model\n",
        "# fasttext_model = fasttext.load_model(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/fasttext/model_bn_300.bin\")\n",
        "\n",
        "# # Function that takes word vector as input and returns an embedding matrix\n",
        "# def embedding_creation(EMBEDDING_DIM, word_vectors, tokenizer):\n",
        "#     vocabulary_size = len(tokenizer.word_index) + 1\n",
        "#     word_index = tokenizer.word_index\n",
        "#     embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "\n",
        "#     for word, i in word_index.items():\n",
        "#         try:\n",
        "#             embedding_vector = word_vectors.get_word_vector(word)\n",
        "#             embedding_matrix[i] = embedding_vector\n",
        "#         except KeyError:\n",
        "#             embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)\n",
        "\n",
        "#     return embedding_matrix\n",
        "\n",
        "# EMBEDDING_DIM = 300\n",
        "\n",
        "# # Replace 'tokenizer' with the tokenizer object you have previously created\n",
        "# # Assuming 'tokenizer' is defined earlier in your code\n",
        "\n",
        "# embedding_matrix = embedding_creation(EMBEDDING_DIM, fasttext_model, tokenizer)\n"
      ],
      "metadata": {
        "id": "7ZtH6vjf0E5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configuration"
      ],
      "metadata": {
        "id": "1MCeNzrXmN_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05EXLtCGl-gG",
        "outputId": "222ca7c3-f914-4fec-a852-924a2467798f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "UwVMKFkYo7-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize and train the SVM classifier\n",
        "# svm_classifier = SVC(kernel='rbf', C=1.0, decision_function_shape='ovr', random_state=42)\n",
        "# svm_classifier.fit(train_padded, train_y)  # Assuming 'train_y' contains the labels\n",
        "\n",
        "# # Make predictions on the validation set\n",
        "# val_predictions = svm_classifier.predict(val_padded)\n",
        "\n",
        "# # Evaluate the model on the validation set\n",
        "# val_accuracy = accuracy_score(val_y, val_predictions)\n",
        "# val_classification_report = classification_report(val_y, val_predictions)\n",
        "\n",
        "# print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "# print(\"Validation Classification Report:\\n\", val_classification_report)\n",
        "\n",
        "# # Make predictions on the test set\n",
        "# test_predictions = svm_classifier.predict(test_padded)\n",
        "\n",
        "# # Evaluate the model on the test set\n",
        "# test_accuracy = accuracy_score(test_y, test_predictions)\n",
        "# test_classification_report = classification_report(test_y, test_predictions)\n",
        "\n",
        "# print(f\"Test Accuracy: {test_accuracy}\")\n",
        "# print(\"Test Classification Report:\\n\", test_classification_report)\n"
      ],
      "metadata": {
        "id": "ImZtU3XEo9vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the pre-trained CBOW Word2Vec model\n",
        "cbow_model = Word2Vec.load(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/final/nafi/cbow_model\")\n",
        "\n",
        "# Assuming you have datasets with text and labels: train_dataset, val_dataset, test_dataset\n",
        "# Replace 'texts' and 'labels' with your data\n",
        "train_texts = train_dataset['text'].tolist()\n",
        "val_texts = val_dataset['text'].tolist()\n",
        "test_texts = test_dataset['text'].tolist()\n",
        "\n",
        "train_labels = train_dataset['label'].tolist()\n",
        "val_labels = val_dataset['label'].tolist()\n",
        "test_labels = test_dataset['label'].tolist()\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts + val_texts + test_texts)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_len = 256  # Set your desired maximum sequence length\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "val_sequences = pad_sequences(val_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# Create arrays to hold the word embeddings for each sequence\n",
        "embedding_dim = cbow_model.vector_size\n",
        "\n",
        "def get_word_embeddings(sequences):\n",
        "    word_embeddings = []\n",
        "    for seq in sequences:\n",
        "        seq_embeddings = []\n",
        "        for token_idx in seq:\n",
        "            word = tokenizer.index_word.get(token_idx, '')\n",
        "            if word in cbow_model.wv:\n",
        "                word_vector = cbow_model.wv[word]\n",
        "            else:\n",
        "                # Handle out-of-vocabulary words\n",
        "                word_vector = np.zeros(embedding_dim)  # Replace with appropriate handling\n",
        "            seq_embeddings.append(word_vector)\n",
        "        word_embeddings.append(seq_embeddings)\n",
        "    return np.array(word_embeddings)\n",
        "\n",
        "# Convert sequences to word embeddings\n",
        "X_train = get_word_embeddings(train_sequences)\n",
        "X_val = get_word_embeddings(val_sequences)\n",
        "X_test = get_word_embeddings(test_sequences)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "# Initialize and train the Logistic Regression classifier on the training data\n",
        "logistic_classifier = LogisticRegression(max_iter=1000)\n",
        "logistic_classifier.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_predictions = logistic_classifier.predict(X_val.reshape(X_val.shape[0], -1))\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "val_classification_report = classification_report(y_val, val_predictions)\n",
        "\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "print(\"Validation Classification Report:\\n\", val_classification_report)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = logistic_classifier.predict(X_test.reshape(X_test.shape[0], -1))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "test_classification_report = classification_report(y_test, test_predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"Test Classification Report:\\n\", test_classification_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuICrRrZ_Pkb",
        "outputId": "0d3c5a3b-8526-47e1-ca89-a79d393fd0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5323308270676692\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.96      0.69       717\n",
            "           1       0.37      0.05      0.08       417\n",
            "           2       0.24      0.02      0.04       196\n",
            "\n",
            "    accuracy                           0.53      1330\n",
            "   macro avg       0.38      0.34      0.27      1330\n",
            "weighted avg       0.44      0.53      0.40      1330\n",
            "\n",
            "Test Accuracy: 0.5391865079365079\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.94      0.69      1096\n",
            "           1       0.45      0.07      0.12       719\n",
            "           2       0.17      0.01      0.03       201\n",
            "\n",
            "    accuracy                           0.54      2016\n",
            "   macro avg       0.39      0.34      0.28      2016\n",
            "weighted avg       0.47      0.54      0.42      2016\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye2o9swY09Lf",
        "outputId": "48a3e6d4-58b6-4db8-a536-23fb4afd6db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Save the trained model\n",
        "model_filename = 'no_embedding_lr_model.pkl'\n",
        "# Define the directory where you want to save the model\n",
        "save_directory =\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/final/nafi/\"\n",
        "# Combine the directory and filename to create the full file path\n",
        "model_path = os.path.join(save_directory, model_filename)\n",
        "\n",
        "joblib.dump(logistic_classifier, model_path)\n",
        "\n",
        "print(f\"Model saved as {model_path}\")"
      ],
      "metadata": {
        "id": "Z_ulPL0i09dp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4576b10a-4c50-4a3c-a56c-c71238a31d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as /content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/final/nafi/no_embedding_lr_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and testing"
      ],
      "metadata": {
        "id": "9GatfNXr29j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the directory and filename where the model is saved\n",
        "model_directory = \"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/final/nafi/\"\n",
        "model_filename = 'no_embedding_lr_model.pkl'\n",
        "\n",
        "# Load the trained logistic regression model\n",
        "model_path = os.path.join(model_directory, model_filename)\n",
        "logistic_classifier2 = joblib.load(model_path)"
      ],
      "metadata": {
        "id": "rsXupMkB2ZOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "test_predictions = logistic_classifier2.predict(X_test.reshape(X_test.shape[0], -1))\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = accuracy_score(test_y, test_predictions)\n",
        "test_classification_report = classification_report(test_y, test_predictions, target_names=['0', '1', '2'], output_dict=True)\n",
        "\n",
        "# Convert the classification report dictionary to a DataFrame\n",
        "report_df = pd.DataFrame.from_dict(test_classification_report).T\n",
        "\n",
        "# Format the DataFrame to display percentages\n",
        "report_df['support'] = report_df['support'].astype(int)\n",
        "report_df['accuracy'] = test_accuracy\n",
        "report_df['accuracy'] = report_df['accuracy'].apply(lambda x: f'{x * 100:.2f}%')\n",
        "report_df.iloc[:-1, :-1] = (report_df.iloc[:-1, :-1] * 100).applymap(lambda x: f'{x:.2f}%')\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "print(\"Test Classification Report:\\n\", report_df)\n"
      ],
      "metadata": {
        "id": "LzGVnzLG3CNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d693b9c-e01e-4355-8e82-764fe7b6370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 53.92%\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score     support accuracy\n",
            "0               54.79%    94.43%    69.35%  109600.00%   53.92%\n",
            "1               44.95%     6.82%    11.84%   71900.00%   53.92%\n",
            "2               16.67%     1.49%     2.74%   20100.00%   53.92%\n",
            "accuracy        53.92%    53.92%    53.92%       0.00%   53.92%\n",
            "macro avg       38.80%    34.25%    27.97%  201600.00%   53.92%\n",
            "weighted avg  0.474816  0.539187  0.421947        2016   53.92%\n"
          ]
        }
      ]
    }
  ]
}