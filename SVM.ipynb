{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPG2OxlyC1YC7hAPXMQWWys",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tariquzzaman-faisal/VITD/blob/main/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mouting Drive"
      ],
      "metadata": {
        "id": "5Qf_4Bppkyr-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7idq270YkgwA",
        "outputId": "fdbba287-dfc4-4561-9e94-161289aa8d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvRx6hopk0Wy",
        "outputId": "e82070a6-ac7b-4d77-d4c2-1fff5c11e266"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgBSLuwmk10Q",
        "outputId": "823072f1-1832-4a0d-992c-32703ccd2b23"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.11.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Input, Flatten, MaxPooling1D, SpatialDropout1D, Activation\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import gensim\n",
        "from gensim import models\n",
        "from gensim.models import Word2Vec\n",
        "import fasttext.util\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CYYXx1IQk3q0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading fasttext model"
      ],
      "metadata": {
        "id": "EzmngP3klFWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model = fasttext.load_model(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/notebooks/Tariq/fasttext/model_bn_300.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIfW9IlrlC1n",
        "outputId": "ce40e5c3-f6e6-46cc-a7cf-ea0310741ced"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset"
      ],
      "metadata": {
        "id": "lph95l_JlKVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/train.csv\")\n",
        "val_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/dev.csv\")\n",
        "test_dataset = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/task datasets/original/test.csv\")"
      ],
      "metadata": {
        "id": "zrcpIBsrlH2O"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Select the first 10 samples from each dataset\n",
        "# train_dataset = train_dataset.head(10)\n",
        "# val_dataset = val_dataset.head(10)\n",
        "# test_dataset = test_dataset.head(10)"
      ],
      "metadata": {
        "id": "rckKnISMu3Fx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train: {train_dataset.shape}\\nval: {val_dataset.shape}\\ntest: {test_dataset.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqAzmieZlLwQ",
        "outputId": "5a2256f5-62f4-4241-b114-4e5243c2cc48"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: (2700, 2)\n",
            "val: (1330, 2)\n",
            "test: (2016, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr-KKxEmlOvd",
        "outputId": "c5e30e37-4a60-455d-9a50-b385f6fe3f95"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1389\n",
              "1     922\n",
              "2     389\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_dataset['text']\n",
        "train_y = train_dataset['label']\n",
        "\n",
        "val_x = val_dataset['text']\n",
        "val_y = val_dataset['label']\n",
        "\n",
        "test_x = test_dataset['text']\n",
        "test_y = test_dataset['label']"
      ],
      "metadata": {
        "id": "MYWdT0z_lQrQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Setup"
      ],
      "metadata": {
        "id": "yKaRdmfcllEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(oov_token = \"<OOV>\", split=' ') # Splitting text based on whitespace and adding \"Out of vocabulary\"\n",
        "tokenizer.fit_on_texts(train_x) # Using the tokenizer on out train dataset to tokenize the train dataset\n",
        "train_encoded=tokenizer.texts_to_sequences(train_x)\n",
        "# print(train_encoded)"
      ],
      "metadata": {
        "id": "aftL0z0WlfI4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded= pad_sequences(train_encoded, padding='post', maxlen=256)\n",
        "# print(train_padded)"
      ],
      "metadata": {
        "id": "UCr9BUHJlm4N"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDl1TsE_lpnZ",
        "outputId": "b0cae5fd-8dd5-4c33-e329-4f220db39c7c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# padding df_test\n",
        "test_encoded=tokenizer.texts_to_sequences(test_x)\n",
        "test_padded= pad_sequences(test_encoded, padding='post', maxlen=train_padded.shape[1])"
      ],
      "metadata": {
        "id": "Od8PRICnlsPV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding df_validation\n",
        "val_encoded=tokenizer.texts_to_sequences(val_x)\n",
        "val_padded= pad_sequences(val_encoded, padding='post', maxlen=train_padded.shape[1])"
      ],
      "metadata": {
        "id": "Xj5OHq5clvIS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that takes word vector as input and returned an embedding layer\n",
        "def embedding_creation(EMBEDDING_DIM, word_vectors):\n",
        "  vocabulary_size=len(tokenizer.word_index)+1\n",
        "  word_index=tokenizer.word_index\n",
        "  embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "\n",
        "  for word, i in word_index.items():\n",
        "    try:\n",
        "      embedding_vector=word_vectors[word] # taking the word vector of all the words in the index\n",
        "      embedding_matrix[i]=embedding_vector # inserting the vector of the word to the embeddings matrix,  index wise\n",
        "    except KeyError:\n",
        "      embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "      \"\"\"\n",
        "      The strategy of generating random vectors for missing words (KeyError)\n",
        "      in the embedding matrix is useful because it provides a way to\n",
        "      include out-of-vocabulary words in the representation,\n",
        "        prevents loss of information, helps with stable training, and\n",
        "        ensures a complete embedding matrix for neural network models.\n",
        "      \"\"\"\n",
        "  embedding_layer=Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)\n",
        "\n",
        "  return embedding_layer"
      ],
      "metadata": {
        "id": "mXwth_Oalxe9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 300\n",
        "wv = fasttext_model\n",
        "IFT = embedding_creation(EMBEDDING_DIM, wv)\n",
        "# gets the embedding layer from the word vectors using EMBEDDING_DIM as dim size"
      ],
      "metadata": {
        "id": "N5lQ-jyMlziZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = train_padded.shape[1]\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "# creating a randomly initialized embedding layer (RE)\n",
        "RE = Embedding(vocabulary_size, EMBEDDING_DIM,input_length = max_length, trainable=True)"
      ],
      "metadata": {
        "id": "u6Sr363Ql1ga"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Stopping"
      ],
      "metadata": {
        "id": "wgYjEM3-mGiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earlystop_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    mode=\"min\",\n",
        "    restore_best_weights=True,\n",
        ")"
      ],
      "metadata": {
        "id": "cR6aDwril3B-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_X_name_collection = [ [IFT, 'IFT']]\n",
        "\"\"\"\n",
        "IFT = embedding_creation(EMBEDDING_DIM, wv)\n",
        "# IFT has the embedding layer from the word vectors using EMBEDDING_DIM as dim size\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vhEFPsuNl82T",
        "outputId": "d28894aa-d5f8-4bce-db6d-031d82817a8e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIFT = embedding_creation(EMBEDDING_DIM, wv)\\n# IFT has the embedding layer from the word vectors using EMBEDDING_DIM as dim size\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configuration"
      ],
      "metadata": {
        "id": "1MCeNzrXmN_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05EXLtCGl-gG",
        "outputId": "e4dba5b0-51f1-4d4a-b7e2-97871332b29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "UwVMKFkYo7-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the SVM classifier for multi-class classification\n",
        "svm_classifier = SVC(kernel='linear', decision_function_shape='ovr')  # 'ovr' stands for \"one-vs-rest\"\n",
        "svm_classifier.fit(train_padded, train_y)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_predictions = svm_classifier.predict(val_padded)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_accuracy = accuracy_score(val_y, val_predictions)\n",
        "val_classification_report = classification_report(val_y, val_predictions)\n",
        "\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "print(\"Validation Classification Report:\\n\", val_classification_report)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = svm_classifier.predict(test_padded)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = accuracy_score(test_y, test_predictions)\n",
        "test_classification_report = classification_report(test_y, test_predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"Test Classification Report:\\n\", test_classification_report)\n"
      ],
      "metadata": {
        "id": "ImZtU3XEo9vE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}