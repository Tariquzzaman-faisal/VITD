{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tariquzzaman-faisal/VITD/blob/main/banglabert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjXm4LgEB8FN"
      },
      "source": [
        "# Project setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVPUsrw9Ac7o",
        "outputId": "0d8d636d-5d68-40e1-dc88-3e2640604817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16AdzvaMFtj5",
        "outputId": "58331acd-4477-4fba-a5e4-1b195c9755ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 15 04:22:36 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCQ9A2EdFwo8",
        "outputId": "f80fd918-c250-40e7-ee1b-3d588f32f310"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XohNnoz5B-zb"
      },
      "source": [
        "# Setting up libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkERq9JwslRo",
        "outputId": "f0368db5-4f13-4115-be63-bdde1ea5171f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7eBAjHcLsnDv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_directory = '/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/Tariq/split/'\n",
        "train_df = pd.read_csv(datasets_directory+'train.csv')\n",
        "test_df = pd.read_csv(datasets_directory+'test.csv')\n",
        "val_df = pd.read_csv(datasets_directory+'validation.csv')"
      ],
      "metadata": {
        "id": "w1pBOY5qstDQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQcYN2zUs4SQ",
        "outputId": "b2c0ce2a-ba82-4bbe-e723-c9d05846b79b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentence1', 'label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3uf6vwMtEdx",
        "outputId": "b037df44-6ace-4e48-f20a-df2243ba57d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentence1', 'label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmnpTmjNtGSA",
        "outputId": "4ee7e04e-9b3a-4915-80aa-d057feedec3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentence1', 'label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape, test_df.shape, val_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbz_4mKGzR_B",
        "outputId": "56edfb99-0a14-4744-da1f-b45dc55d61d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2700, 2), (2016, 1), (1330, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape, test_df.shape, val_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eir2jzc_zNpn",
        "outputId": "df2e6560-642d-4b05-e399-f4451ec6decd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2700, 2), (2016, 1), (1330, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/csebuetnlp/normalizer transformers torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz_DGVtVX1Sl",
        "outputId": "98bd0c06-9b9d-46a0-a23d-b6e9b35588fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/csebuetnlp/normalizer\n",
            "  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-4watdgm1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-4watdgm1\n",
            "  Resolved https://github.com/csebuetnlp/normalizer to commit d405944dde5ceeacb7c2fd3245ae2a9dea5f35c9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (2023.6.3)\n",
            "Collecting emoji==1.4.2 (from normalizer==0.0.1)\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy==6.0.3 (from normalizer==0.0.1)\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: normalizer, emoji, ftfy\n",
            "  Building wheel for normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for normalizer: filename=normalizer-0.0.1-py3-none-any.whl size=6861 sha256=1859cf7b2fbfb882a21fea71129a97c217edb0ec3f79fd60962387e7268f92b4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ldew78mj/wheels/2e/79/9c/cd96d490298305d51d2da11484bb2c25fd1f759a6906708282\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186456 sha256=a221dfeab0d52bdd5d53b14dc8eb6bd79b1078826bd4d13fe0241a380b5e844f\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41934 sha256=fa88ff632fb50a9bb8154cfd7a02150942469c67b3c8cdde31e72abdc8d93b6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/8e/16/c1e4d4d65685d71085e4e27b44d6ed880b0559474c9ee4ff66\n",
            "Successfully built normalizer emoji ftfy\n",
            "Installing collected packages: emoji, ftfy, normalizer\n",
            "Successfully installed emoji-1.4.2 ftfy-6.0.3 normalizer-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from normalizer import normalize"
      ],
      "metadata": {
        "id": "Fk-PMUgvF2S6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification"
      ],
      "metadata": {
        "id": "fNO_ZW1DIQeJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "OT68bt4gItPZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/csebuetnlp/banglabert.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2NcqCqFJCD3",
        "outputId": "ea333deb-7c97-4f5c-da3f-664982410cf7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'banglabert'...\n",
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 141 (delta 70), reused 73 (delta 23), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (141/141), 1.11 MiB | 4.57 MiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls banglabert/sequence_classification/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw4x_8y04qGs",
        "outputId": "aa1e3072-90a2-453a-ca01-2ad3ee5e4e31"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluate.sh  README.md  \u001b[0m\u001b[01;34msample_inputs\u001b[0m/  sequence_classification.py  trainer.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python banglabert/sequence_classification/sequence_classification.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iQWTV7RJMvt",
        "outputId": "adc8cbd9-bb4d-4934-e782-0c4c8e93c77b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-15 04:23:46.755122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: sequence_classification.py\n",
            "       [-h]\n",
            "       --model_name_or_path\n",
            "       MODEL_NAME_OR_PATH\n",
            "       [--cache_dir CACHE_DIR]\n",
            "       [--dataset_dir DATASET_DIR]\n",
            "       [--max_seq_length MAX_SEQ_LENGTH]\n",
            "       [--overwrite_cache [OVERWRITE_CACHE]]\n",
            "       [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n",
            "       [--max_train_samples MAX_TRAIN_SAMPLES]\n",
            "       [--max_eval_samples MAX_EVAL_SAMPLES]\n",
            "       [--max_predict_samples MAX_PREDICT_SAMPLES]\n",
            "       [--train_file TRAIN_FILE]\n",
            "       [--validation_file VALIDATION_FILE]\n",
            "       [--test_file TEST_FILE]\n",
            "       [--do_normalize [DO_NORMALIZE]]\n",
            "       [--no_do_normalize]\n",
            "       [--unicode_norm UNICODE_NORM]\n",
            "       [--remove_punct [REMOVE_PUNCT]]\n",
            "       [--remove_emoji [REMOVE_EMOJI]]\n",
            "       [--remove_urls [REMOVE_URLS]]\n",
            "       [--sentence1_key SENTENCE1_KEY]\n",
            "       [--sentence2_key SENTENCE2_KEY]\n",
            "       [--label_key LABEL_KEY]\n",
            "       --output_dir\n",
            "       OUTPUT_DIR\n",
            "       [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "       [--do_train [DO_TRAIN]]\n",
            "       [--do_eval [DO_EVAL]]\n",
            "       [--do_predict [DO_PREDICT]]\n",
            "       [--evaluation_strategy {no,steps,epoch}]\n",
            "       [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "       [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "       [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "       [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "       [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "       [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "       [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "       [--eval_delay EVAL_DELAY]\n",
            "       [--learning_rate LEARNING_RATE]\n",
            "       [--weight_decay WEIGHT_DECAY]\n",
            "       [--adam_beta1 ADAM_BETA1]\n",
            "       [--adam_beta2 ADAM_BETA2]\n",
            "       [--adam_epsilon ADAM_EPSILON]\n",
            "       [--max_grad_norm MAX_GRAD_NORM]\n",
            "       [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "       [--max_steps MAX_STEPS]\n",
            "       [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau}]\n",
            "       [--warmup_ratio WARMUP_RATIO]\n",
            "       [--warmup_steps WARMUP_STEPS]\n",
            "       [--log_level {debug,info,warning,error,critical,passive}]\n",
            "       [--log_level_replica {debug,info,warning,error,critical,passive}]\n",
            "       [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
            "       [--no_log_on_each_node]\n",
            "       [--logging_dir LOGGING_DIR]\n",
            "       [--logging_strategy {no,steps,epoch}]\n",
            "       [--logging_first_step [LOGGING_FIRST_STEP]]\n",
            "       [--logging_steps LOGGING_STEPS]\n",
            "       [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
            "       [--no_logging_nan_inf_filter]\n",
            "       [--save_strategy {no,steps,epoch}]\n",
            "       [--save_steps SAVE_STEPS]\n",
            "       [--save_total_limit SAVE_TOTAL_LIMIT]\n",
            "       [--save_safetensors [SAVE_SAFETENSORS]]\n",
            "       [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
            "       [--no_cuda [NO_CUDA]]\n",
            "       [--use_mps_device [USE_MPS_DEVICE]]\n",
            "       [--seed SEED]\n",
            "       [--data_seed DATA_SEED]\n",
            "       [--jit_mode_eval [JIT_MODE_EVAL]]\n",
            "       [--use_ipex [USE_IPEX]]\n",
            "       [--bf16 [BF16]]\n",
            "       [--fp16 [FP16]]\n",
            "       [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "       [--half_precision_backend {auto,cuda_amp,apex,cpu_amp}]\n",
            "       [--bf16_full_eval [BF16_FULL_EVAL]]\n",
            "       [--fp16_full_eval [FP16_FULL_EVAL]]\n",
            "       [--tf32 TF32]\n",
            "       [--local_rank LOCAL_RANK]\n",
            "       [--ddp_backend {nccl,gloo,mpi,ccl}]\n",
            "       [--tpu_num_cores TPU_NUM_CORES]\n",
            "       [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
            "       [--debug DEBUG [DEBUG ...]]\n",
            "       [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "       [--eval_steps EVAL_STEPS]\n",
            "       [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "       [--past_index PAST_INDEX]\n",
            "       [--run_name RUN_NAME]\n",
            "       [--disable_tqdm DISABLE_TQDM]\n",
            "       [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "       [--no_remove_unused_columns]\n",
            "       [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "       [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "       [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "       [--greater_is_better GREATER_IS_BETTER]\n",
            "       [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
            "       [--sharded_ddp SHARDED_DDP]\n",
            "       [--fsdp FSDP]\n",
            "       [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
            "       [--fsdp_config FSDP_CONFIG]\n",
            "       [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
            "       [--deepspeed DEEPSPEED]\n",
            "       [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "       [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit}]\n",
            "       [--optim_args OPTIM_ARGS]\n",
            "       [--adafactor [ADAFACTOR]]\n",
            "       [--group_by_length [GROUP_BY_LENGTH]]\n",
            "       [--length_column_name LENGTH_COLUMN_NAME]\n",
            "       [--report_to REPORT_TO [REPORT_TO ...]]\n",
            "       [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "       [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
            "       [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n",
            "       [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "       [--no_dataloader_pin_memory]\n",
            "       [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
            "       [--no_skip_memory_metrics]\n",
            "       [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
            "       [--push_to_hub [PUSH_TO_HUB]]\n",
            "       [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "       [--hub_model_id HUB_MODEL_ID]\n",
            "       [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
            "       [--hub_token HUB_TOKEN]\n",
            "       [--hub_private_repo [HUB_PRIVATE_REPO]]\n",
            "       [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
            "       [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
            "       [--fp16_backend {auto,cuda_amp,apex,cpu_amp}]\n",
            "       [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
            "       [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
            "       [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
            "       [--mp_parameters MP_PARAMETERS]\n",
            "       [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
            "       [--full_determinism [FULL_DETERMINISM]]\n",
            "       [--torchdynamo TORCHDYNAMO]\n",
            "       [--ray_scope RAY_SCOPE]\n",
            "       [--ddp_timeout DDP_TIMEOUT]\n",
            "       [--torch_compile [TORCH_COMPILE]]\n",
            "       [--torch_compile_backend TORCH_COMPILE_BACKEND]\n",
            "       [--torch_compile_mode TORCH_COMPILE_MODE]\n",
            "       [--xpu_backend {mpi,ccl,gloo}]\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "    Path to\n",
            "    pretrained\n",
            "    model or\n",
            "    model\n",
            "    identifier\n",
            "    from huggin\n",
            "    gface.co/mo\n",
            "    dels\n",
            "    (default:\n",
            "    None)\n",
            "  --cache_dir CACHE_DIR\n",
            "    Where do\n",
            "    you want to\n",
            "    store the\n",
            "    pretrained\n",
            "    models\n",
            "    downloaded\n",
            "    from huggin\n",
            "    gface.co\n",
            "    (default:\n",
            "    None)\n",
            "  --dataset_dir DATASET_DIR\n",
            "    Path to the\n",
            "    directory\n",
            "    containing\n",
            "    the data\n",
            "    files.\n",
            "    (.csv /\n",
            "    .tsv /\n",
            "    .jsonl)File\n",
            "    datatypes\n",
            "    will be\n",
            "    identified\n",
            "    with their\n",
            "    prefix\n",
            "    names as\n",
            "    follows:\n",
            "    `train`-\n",
            "    Training\n",
            "    file(s)\n",
            "    e.g. `train\n",
            "    .csv`/ `tra\n",
            "    in_part1.cs\n",
            "    v` etc. `va\n",
            "    lidation`-\n",
            "    Evaluation\n",
            "    file(s)\n",
            "    e.g. `valid\n",
            "    ation.csv`/\n",
            "    `validation\n",
            "    _part1.csv`\n",
            "    etc.\n",
            "    `test`-\n",
            "    Test\n",
            "    file(s)\n",
            "    e.g.\n",
            "    `test.csv`/\n",
            "    `test_part1\n",
            "    .csv` etc.\n",
            "    All files\n",
            "    for must\n",
            "    have the\n",
            "    same\n",
            "    extension.\n",
            "    (default:\n",
            "    None)\n",
            "  --max_seq_length MAX_SEQ_LENGTH\n",
            "    The maximum\n",
            "    total input\n",
            "    sequence\n",
            "    length\n",
            "    after token\n",
            "    ization.\n",
            "    Sequences\n",
            "    longer than\n",
            "    this will\n",
            "    be\n",
            "    truncated,\n",
            "    sequences\n",
            "    shorter\n",
            "    will be\n",
            "    padded.\n",
            "    (default:\n",
            "    512)\n",
            "  --overwrite_cache [OVERWRITE_CACHE]\n",
            "    Overwrite\n",
            "    the cached \n",
            "    preprocesse\n",
            "    d datasets\n",
            "    or not.\n",
            "    (default:\n",
            "    False)\n",
            "  --pad_to_max_length [PAD_TO_MAX_LENGTH]\n",
            "    Whether to\n",
            "    pad all\n",
            "    samples to \n",
            "    `max_seq_le\n",
            "    ngth`. If\n",
            "    False, will\n",
            "    pad the\n",
            "    samples\n",
            "    dynamically\n",
            "    when\n",
            "    batching to\n",
            "    the maximum\n",
            "    length in\n",
            "    the batch.\n",
            "    (default:\n",
            "    False)\n",
            "  --max_train_samples MAX_TRAIN_SAMPLES\n",
            "    For\n",
            "    debugging\n",
            "    purposes or\n",
            "    quicker\n",
            "    training,\n",
            "    truncate\n",
            "    the number\n",
            "    of training\n",
            "    examples to\n",
            "    this value\n",
            "    if set.\n",
            "    (default:\n",
            "    None)\n",
            "  --max_eval_samples MAX_EVAL_SAMPLES\n",
            "    For\n",
            "    debugging\n",
            "    purposes or\n",
            "    quicker\n",
            "    training,\n",
            "    truncate\n",
            "    the number\n",
            "    of\n",
            "    evaluation\n",
            "    examples to\n",
            "    this value\n",
            "    if set.\n",
            "    (default:\n",
            "    None)\n",
            "  --max_predict_samples MAX_PREDICT_SAMPLES\n",
            "    For\n",
            "    debugging\n",
            "    purposes or\n",
            "    quicker\n",
            "    training,\n",
            "    truncate\n",
            "    the number\n",
            "    of\n",
            "    prediction\n",
            "    examples to\n",
            "    this value\n",
            "    if set.\n",
            "    (default:\n",
            "    None)\n",
            "  --train_file TRAIN_FILE\n",
            "    A csv / tsv\n",
            "    / jsonl\n",
            "    file\n",
            "    containing\n",
            "    the\n",
            "    training\n",
            "    data.\n",
            "    (default:\n",
            "    None)\n",
            "  --validation_file VALIDATION_FILE\n",
            "    A csv / tsv\n",
            "    / jsonl\n",
            "    file\n",
            "    containing\n",
            "    the\n",
            "    validation\n",
            "    data.\n",
            "    (default:\n",
            "    None)\n",
            "  --test_file TEST_FILE\n",
            "    A csv / tsv\n",
            "    / jsonl\n",
            "    file\n",
            "    containing\n",
            "    the test\n",
            "    data.\n",
            "    (default:\n",
            "    None)\n",
            "  --do_normalize [DO_NORMALIZE]\n",
            "    Normalize\n",
            "    text before\n",
            "    feeding to\n",
            "    the model.\n",
            "    (default:\n",
            "    True)\n",
            "  --no_do_normalize\n",
            "    Normalize\n",
            "    text before\n",
            "    feeding to\n",
            "    the model.\n",
            "    (default:\n",
            "    False)\n",
            "  --unicode_norm UNICODE_NORM\n",
            "    Type of\n",
            "    unicode nor\n",
            "    malization\n",
            "    (default:\n",
            "    NFKC)\n",
            "  --remove_punct [REMOVE_PUNCT]\n",
            "    Remove\n",
            "    punctuation\n",
            "    during norm\n",
            "    alization.\n",
            "    To replace\n",
            "    with custom\n",
            "    token /\n",
            "    selective\n",
            "    replacement\n",
            "    you should\n",
            "    use this\n",
            "    repo (https\n",
            "    ://github.c\n",
            "    om/abhik150\n",
            "    5040/normal\n",
            "    izer)\n",
            "    before\n",
            "    feeding the\n",
            "    data to the\n",
            "    script.\n",
            "    (default:\n",
            "    False)\n",
            "  --remove_emoji [REMOVE_EMOJI]\n",
            "    Remove\n",
            "    emojis\n",
            "    during norm\n",
            "    alization.\n",
            "    To replace\n",
            "    with custom\n",
            "    token /\n",
            "    selective\n",
            "    replacement\n",
            "    you should\n",
            "    use this\n",
            "    repo (https\n",
            "    ://github.c\n",
            "    om/abhik150\n",
            "    5040/normal\n",
            "    izer)\n",
            "    before\n",
            "    feeding the\n",
            "    data to the\n",
            "    script.\n",
            "    (default:\n",
            "    False)\n",
            "  --remove_urls [REMOVE_URLS]\n",
            "    Remove urls\n",
            "    during norm\n",
            "    alization.\n",
            "    To replace\n",
            "    with custom\n",
            "    token /\n",
            "    selective\n",
            "    replacement\n",
            "    you should\n",
            "    use this\n",
            "    repo (https\n",
            "    ://github.c\n",
            "    om/abhik150\n",
            "    5040/normal\n",
            "    izer)\n",
            "    before\n",
            "    feeding the\n",
            "    data to the\n",
            "    script.\n",
            "    (default:\n",
            "    False)\n",
            "  --sentence1_key SENTENCE1_KEY\n",
            "    Key /\n",
            "    column name\n",
            "    in the\n",
            "    input file \n",
            "    correspondi\n",
            "    ng to the\n",
            "    first input\n",
            "    sequence\n",
            "    (default:\n",
            "    sentence1)\n",
            "  --sentence2_key SENTENCE2_KEY\n",
            "    Key /\n",
            "    column name\n",
            "    in the\n",
            "    input file \n",
            "    correspondi\n",
            "    ng to the\n",
            "    second\n",
            "    input\n",
            "    sequence\n",
            "    (default:\n",
            "    sentence2)\n",
            "  --label_key LABEL_KEY\n",
            "    Key /\n",
            "    column name\n",
            "    in the\n",
            "    input file \n",
            "    correspondi\n",
            "    ng to the c\n",
            "    lassificati\n",
            "    on label\n",
            "    (default:\n",
            "    label)\n",
            "  --output_dir OUTPUT_DIR\n",
            "    The output\n",
            "    directory\n",
            "    where the\n",
            "    model\n",
            "    predictions\n",
            "    and\n",
            "    checkpoints\n",
            "    will be\n",
            "    written.\n",
            "    (default:\n",
            "    None)\n",
            "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
            "    Overwrite\n",
            "    the content\n",
            "    of the\n",
            "    output\n",
            "    directory.\n",
            "    Use this to\n",
            "    continue\n",
            "    training if\n",
            "    output_dir\n",
            "    points to a\n",
            "    checkpoint\n",
            "    directory.\n",
            "    (default:\n",
            "    False)\n",
            "  --do_train [DO_TRAIN]\n",
            "    Whether to\n",
            "    run\n",
            "    training.\n",
            "    (default:\n",
            "    False)\n",
            "  --do_eval [DO_EVAL]\n",
            "    Whether to\n",
            "    run eval on\n",
            "    the dev\n",
            "    set.\n",
            "    (default:\n",
            "    False)\n",
            "  --do_predict [DO_PREDICT]\n",
            "    Whether to\n",
            "    run\n",
            "    predictions\n",
            "    on the test\n",
            "    set.\n",
            "    (default:\n",
            "    False)\n",
            "  --evaluation_strategy {no,steps,epoch}\n",
            "    The\n",
            "    evaluation\n",
            "    strategy to\n",
            "    use.\n",
            "    (default:\n",
            "    no)\n",
            "  --prediction_loss_only [PREDICTION_LOSS_ONLY]\n",
            "    When\n",
            "    performing\n",
            "    evaluation\n",
            "    and predict\n",
            "    ions, only\n",
            "    returns the\n",
            "    loss.\n",
            "    (default:\n",
            "    False)\n",
            "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
            "    Batch size\n",
            "    per GPU/TPU\n",
            "    core/CPU\n",
            "    for\n",
            "    training.\n",
            "    (default:\n",
            "    8)\n",
            "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
            "    Batch size\n",
            "    per GPU/TPU\n",
            "    core/CPU\n",
            "    for\n",
            "    evaluation.\n",
            "    (default:\n",
            "    8)\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "    Deprecated,\n",
            "    the use of \n",
            "    `--\n",
            "    per_device_\n",
            "    train_batch\n",
            "    _size` is\n",
            "    preferred.\n",
            "    Batch size\n",
            "    per GPU/TPU\n",
            "    core/CPU\n",
            "    for\n",
            "    training.\n",
            "    (default:\n",
            "    None)\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "    Deprecated,\n",
            "    the use of \n",
            "    `--\n",
            "    per_device_\n",
            "    eval_batch_\n",
            "    size` is\n",
            "    preferred.\n",
            "    Batch size\n",
            "    per GPU/TPU\n",
            "    core/CPU\n",
            "    for\n",
            "    evaluation.\n",
            "    (default:\n",
            "    None)\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "    Number of\n",
            "    updates\n",
            "    steps to\n",
            "    accumulate\n",
            "    before\n",
            "    performing\n",
            "    a backward/\n",
            "    update\n",
            "    pass.\n",
            "    (default:\n",
            "    1)\n",
            "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
            "    Number of\n",
            "    predictions\n",
            "    steps to\n",
            "    accumulate\n",
            "    before\n",
            "    moving the\n",
            "    tensors to\n",
            "    the CPU.\n",
            "    (default:\n",
            "    None)\n",
            "  --eval_delay EVAL_DELAY\n",
            "    Number of\n",
            "    epochs or\n",
            "    steps to\n",
            "    wait for\n",
            "    before the\n",
            "    first\n",
            "    evaluation\n",
            "    can be\n",
            "    performed,\n",
            "    depending\n",
            "    on the eval\n",
            "    uation_stra\n",
            "    tegy.\n",
            "    (default:\n",
            "    0)\n",
            "  --learning_rate LEARNING_RATE\n",
            "    The initial\n",
            "    learning\n",
            "    rate for\n",
            "    AdamW.\n",
            "    (default:\n",
            "    5e-05)\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "    Weight\n",
            "    decay for\n",
            "    AdamW if we\n",
            "    apply some.\n",
            "    (default:\n",
            "    0.0)\n",
            "  --adam_beta1 ADAM_BETA1\n",
            "    Beta1 for\n",
            "    AdamW\n",
            "    optimizer\n",
            "    (default:\n",
            "    0.9)\n",
            "  --adam_beta2 ADAM_BETA2\n",
            "    Beta2 for\n",
            "    AdamW\n",
            "    optimizer\n",
            "    (default:\n",
            "    0.999)\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "    Epsilon for\n",
            "    AdamW\n",
            "    optimizer.\n",
            "    (default:\n",
            "    1e-08)\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "    Max\n",
            "    gradient\n",
            "    norm.\n",
            "    (default:\n",
            "    1.0)\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "    Total\n",
            "    number of\n",
            "    training\n",
            "    epochs to\n",
            "    perform.\n",
            "    (default:\n",
            "    3.0)\n",
            "  --max_steps MAX_STEPS\n",
            "    If > 0: set\n",
            "    total\n",
            "    number of\n",
            "    training\n",
            "    steps to\n",
            "    perform.\n",
            "    Override nu\n",
            "    m_train_epo\n",
            "    chs.\n",
            "    (default:\n",
            "    -1)\n",
            "  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau}\n",
            "    The\n",
            "    scheduler\n",
            "    type to\n",
            "    use.\n",
            "    (default:\n",
            "    linear)\n",
            "  --warmup_ratio WARMUP_RATIO\n",
            "    Linear\n",
            "    warmup over\n",
            "    warmup_rati\n",
            "    o fraction\n",
            "    of total\n",
            "    steps.\n",
            "    (default:\n",
            "    0.0)\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "    Linear\n",
            "    warmup over\n",
            "    warmup_step\n",
            "    s.\n",
            "    (default:\n",
            "    0)\n",
            "  --log_level {debug,info,warning,error,critical,passive}\n",
            "    Logger log\n",
            "    level to\n",
            "    use on the\n",
            "    main node.\n",
            "    Possible\n",
            "    choices are\n",
            "    the log\n",
            "    levels as\n",
            "    strings:\n",
            "    'debug',\n",
            "    'info',\n",
            "    'warning',\n",
            "    'error' and\n",
            "    'critical',\n",
            "    plus a\n",
            "    'passive'\n",
            "    level which\n",
            "    doesn't set\n",
            "    anything\n",
            "    and lets\n",
            "    the\n",
            "    application\n",
            "    set the\n",
            "    level.\n",
            "    Defaults to\n",
            "    'passive'.\n",
            "    (default:\n",
            "    passive)\n",
            "  --log_level_replica {debug,info,warning,error,critical,passive}\n",
            "    Logger log\n",
            "    level to\n",
            "    use on\n",
            "    replica\n",
            "    nodes. Same\n",
            "    choices and\n",
            "    defaults as\n",
            "    ``log_level\n",
            "    ``\n",
            "    (default:\n",
            "    warning)\n",
            "  --log_on_each_node [LOG_ON_EACH_NODE]\n",
            "    When doing\n",
            "    a multinode\n",
            "    distributed\n",
            "    training,\n",
            "    whether to\n",
            "    log once\n",
            "    per node or\n",
            "    just once\n",
            "    on the main\n",
            "    node.\n",
            "    (default:\n",
            "    True)\n",
            "  --no_log_on_each_node\n",
            "    When doing\n",
            "    a multinode\n",
            "    distributed\n",
            "    training,\n",
            "    whether to\n",
            "    log once\n",
            "    per node or\n",
            "    just once\n",
            "    on the main\n",
            "    node.\n",
            "    (default:\n",
            "    False)\n",
            "  --logging_dir LOGGING_DIR\n",
            "    Tensorboard\n",
            "    log dir.\n",
            "    (default:\n",
            "    None)\n",
            "  --logging_strategy {no,steps,epoch}\n",
            "    The logging\n",
            "    strategy to\n",
            "    use.\n",
            "    (default:\n",
            "    steps)\n",
            "  --logging_first_step [LOGGING_FIRST_STEP]\n",
            "    Log the\n",
            "    first\n",
            "    global_step\n",
            "    (default:\n",
            "    False)\n",
            "  --logging_steps LOGGING_STEPS\n",
            "    Log every X\n",
            "    updates\n",
            "    steps.\n",
            "    Should be\n",
            "    an integer\n",
            "    or a float\n",
            "    in range\n",
            "    `[0,1)`.If\n",
            "    smaller\n",
            "    than 1,\n",
            "    will be\n",
            "    interpreted\n",
            "    as ratio of\n",
            "    total\n",
            "    training\n",
            "    steps.\n",
            "    (default:\n",
            "    500)\n",
            "  --logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]\n",
            "    Filter nan\n",
            "    and inf\n",
            "    losses for\n",
            "    logging.\n",
            "    (default:\n",
            "    True)\n",
            "  --no_logging_nan_inf_filter\n",
            "    Filter nan\n",
            "    and inf\n",
            "    losses for\n",
            "    logging.\n",
            "    (default:\n",
            "    False)\n",
            "  --save_strategy {no,steps,epoch}\n",
            "    The\n",
            "    checkpoint\n",
            "    save\n",
            "    strategy to\n",
            "    use.\n",
            "    (default:\n",
            "    steps)\n",
            "  --save_steps SAVE_STEPS\n",
            "    Save\n",
            "    checkpoint\n",
            "    every X\n",
            "    updates\n",
            "    steps.\n",
            "    Should be\n",
            "    an integer\n",
            "    or a float\n",
            "    in range\n",
            "    `[0,1)`.If\n",
            "    smaller\n",
            "    than 1,\n",
            "    will be\n",
            "    interpreted\n",
            "    as ratio of\n",
            "    total\n",
            "    training\n",
            "    steps.\n",
            "    (default:\n",
            "    500)\n",
            "  --save_total_limit SAVE_TOTAL_LIMIT\n",
            "    If a value\n",
            "    is passed,\n",
            "    will limit\n",
            "    the total\n",
            "    amount of c\n",
            "    heckpoints.\n",
            "    Deletes the\n",
            "    older\n",
            "    checkpoints\n",
            "    in `output_\n",
            "    dir`. When \n",
            "    `load_best_\n",
            "    model_at_en\n",
            "    d` is\n",
            "    enabled,\n",
            "    the 'best'\n",
            "    checkpoint\n",
            "    according\n",
            "    to `metric_\n",
            "    for_best_mo\n",
            "    del` will\n",
            "    always be\n",
            "    retained in\n",
            "    addition to\n",
            "    the most\n",
            "    recent\n",
            "    ones. For\n",
            "    example,\n",
            "    for `save_t\n",
            "    otal_limit=\n",
            "    5` and `loa\n",
            "    d_best_mode\n",
            "    l_at_end=Tr\n",
            "    ue`, the\n",
            "    four last\n",
            "    checkpoints\n",
            "    will always\n",
            "    be retained\n",
            "    alongside\n",
            "    the best\n",
            "    model. When\n",
            "    `save_total\n",
            "    _limit=1`\n",
            "    and `load_b\n",
            "    est_model_a\n",
            "    t_end=True`\n",
            "    , it is\n",
            "    possible\n",
            "    that two\n",
            "    checkpoints\n",
            "    are saved:\n",
            "    the last\n",
            "    one and the\n",
            "    best one\n",
            "    (if they\n",
            "    are\n",
            "    different).\n",
            "    Default is\n",
            "    unlimited\n",
            "    checkpoints\n",
            "    (default:\n",
            "    None)\n",
            "  --save_safetensors [SAVE_SAFETENSORS]\n",
            "    Use\n",
            "    safetensors\n",
            "    saving and\n",
            "    loading for\n",
            "    state dicts\n",
            "    instead of\n",
            "    default\n",
            "    torch.load\n",
            "    and\n",
            "    torch.save.\n",
            "    (default:\n",
            "    False)\n",
            "  --save_on_each_node [SAVE_ON_EACH_NODE]\n",
            "    When doing\n",
            "    multi-node\n",
            "    distributed\n",
            "    training,\n",
            "    whether to\n",
            "    save models\n",
            "    and\n",
            "    checkpoints\n",
            "    on each\n",
            "    node, or\n",
            "    only on the\n",
            "    main one\n",
            "    (default:\n",
            "    False)\n",
            "  --no_cuda [NO_CUDA]\n",
            "    Do not use\n",
            "    CUDA even\n",
            "    when it is\n",
            "    available\n",
            "    (default:\n",
            "    False)\n",
            "  --use_mps_device [USE_MPS_DEVICE]\n",
            "    This\n",
            "    argument is\n",
            "    deprecated.\n",
            "    `mps`\n",
            "    device will\n",
            "    be used if\n",
            "    available\n",
            "    similar to\n",
            "    `cuda`\n",
            "    device. It\n",
            "    will be\n",
            "    removed in\n",
            "    version 5.0\n",
            "    of 🤗 Transf\n",
            "    ormers\n",
            "    (default:\n",
            "    False)\n",
            "  --seed SEED\n",
            "    Random seed\n",
            "    that will\n",
            "    be set at\n",
            "    the\n",
            "    beginning\n",
            "    of\n",
            "    training.\n",
            "    (default:\n",
            "    42)\n",
            "  --data_seed DATA_SEED\n",
            "    Random seed\n",
            "    to be used\n",
            "    with data\n",
            "    samplers.\n",
            "    (default:\n",
            "    None)\n",
            "  --jit_mode_eval [JIT_MODE_EVAL]\n",
            "    Whether or\n",
            "    not to use\n",
            "    PyTorch jit\n",
            "    trace for\n",
            "    inference\n",
            "    (default:\n",
            "    False)\n",
            "  --use_ipex [USE_IPEX]\n",
            "    Use Intel\n",
            "    extension\n",
            "    for PyTorch\n",
            "    when it is\n",
            "    available, \n",
            "    installatio\n",
            "    n: 'https:/\n",
            "    /github.com\n",
            "    /intel/inte\n",
            "    l-\n",
            "    extension-\n",
            "    for-\n",
            "    pytorch'\n",
            "    (default:\n",
            "    False)\n",
            "  --bf16 [BF16]\n",
            "    Whether to\n",
            "    use bf16\n",
            "    (mixed)\n",
            "    precision\n",
            "    instead of\n",
            "    32-bit.\n",
            "    Requires\n",
            "    Ampere or\n",
            "    higher\n",
            "    NVIDIA arch\n",
            "    itecture or\n",
            "    using CPU\n",
            "    (no_cuda).\n",
            "    This is an \n",
            "    experimenta\n",
            "    l API and\n",
            "    it may\n",
            "    change.\n",
            "    (default:\n",
            "    False)\n",
            "  --fp16 [FP16]\n",
            "    Whether to\n",
            "    use fp16\n",
            "    (mixed)\n",
            "    precision\n",
            "    instead of\n",
            "    32-bit\n",
            "    (default:\n",
            "    False)\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "    For fp16:\n",
            "    Apex AMP op\n",
            "    timization\n",
            "    level\n",
            "    selected in\n",
            "    ['O0',\n",
            "    'O1', 'O2',\n",
            "    and 'O3'].\n",
            "    See details\n",
            "    at https://\n",
            "    nvidia.gith\n",
            "    ub.io/apex/\n",
            "    amp.html\n",
            "    (default:\n",
            "    O1)\n",
            "  --half_precision_backend {auto,cuda_amp,apex,cpu_amp}\n",
            "    The backend\n",
            "    to be used\n",
            "    for half\n",
            "    precision.\n",
            "    (default:\n",
            "    auto)\n",
            "  --bf16_full_eval [BF16_FULL_EVAL]\n",
            "    Whether to\n",
            "    use full\n",
            "    bfloat16\n",
            "    evaluation\n",
            "    instead of\n",
            "    32-bit.\n",
            "    This is an \n",
            "    experimenta\n",
            "    l API and\n",
            "    it may\n",
            "    change.\n",
            "    (default:\n",
            "    False)\n",
            "  --fp16_full_eval [FP16_FULL_EVAL]\n",
            "    Whether to\n",
            "    use full\n",
            "    float16\n",
            "    evaluation\n",
            "    instead of\n",
            "    32-bit\n",
            "    (default:\n",
            "    False)\n",
            "  --tf32 TF32\n",
            "    Whether to\n",
            "    enable tf32\n",
            "    mode,\n",
            "    available\n",
            "    in Ampere\n",
            "    and newer\n",
            "    GPU archite\n",
            "    ctures.\n",
            "    This is an \n",
            "    experimenta\n",
            "    l API and\n",
            "    it may\n",
            "    change.\n",
            "    (default:\n",
            "    None)\n",
            "  --local_rank LOCAL_RANK\n",
            "    For\n",
            "    distributed\n",
            "    training:\n",
            "    local_rank\n",
            "    (default:\n",
            "    -1)\n",
            "  --ddp_backend {nccl,gloo,mpi,ccl}\n",
            "    The backend\n",
            "    to be used\n",
            "    for\n",
            "    distributed\n",
            "    training\n",
            "    (default:\n",
            "    None)\n",
            "  --tpu_num_cores TPU_NUM_CORES\n",
            "    TPU: Number\n",
            "    of TPU\n",
            "    cores (auto\n",
            "    matically\n",
            "    passed by\n",
            "    launcher\n",
            "    script)\n",
            "    (default:\n",
            "    None)\n",
            "  --tpu_metrics_debug [TPU_METRICS_DEBUG]\n",
            "    Deprecated,\n",
            "    the use of\n",
            "    `--debug tp\n",
            "    u_metrics_d\n",
            "    ebug` is\n",
            "    preferred.\n",
            "    TPU:\n",
            "    Whether to\n",
            "    print debug\n",
            "    metrics\n",
            "    (default:\n",
            "    False)\n",
            "  --debug DEBUG [DEBUG ...]\n",
            "    Whether or\n",
            "    not to\n",
            "    enable\n",
            "    debug mode.\n",
            "    Current\n",
            "    options: `u\n",
            "    nderflow_ov\n",
            "    erflow`\n",
            "    (Detect\n",
            "    underflow\n",
            "    and\n",
            "    overflow in\n",
            "    activations\n",
            "    and\n",
            "    weights), `\n",
            "    tpu_metrics\n",
            "    _debug`\n",
            "    (print\n",
            "    debug\n",
            "    metrics on\n",
            "    TPU).\n",
            "    (default:\n",
            "    None)\n",
            "  --dataloader_drop_last [DATALOADER_DROP_LAST]\n",
            "    Drop the\n",
            "    last\n",
            "    incomplete\n",
            "    batch if it\n",
            "    is not\n",
            "    divisible\n",
            "    by the\n",
            "    batch size.\n",
            "    (default:\n",
            "    False)\n",
            "  --eval_steps EVAL_STEPS\n",
            "    Run an\n",
            "    evaluation\n",
            "    every X\n",
            "    steps.\n",
            "    Should be\n",
            "    an integer\n",
            "    or a float\n",
            "    in range\n",
            "    `[0,1)`.If\n",
            "    smaller\n",
            "    than 1,\n",
            "    will be\n",
            "    interpreted\n",
            "    as ratio of\n",
            "    total\n",
            "    training\n",
            "    steps.\n",
            "    (default:\n",
            "    None)\n",
            "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
            "    Number of s\n",
            "    ubprocesses\n",
            "    to use for\n",
            "    data\n",
            "    loading\n",
            "    (PyTorch\n",
            "    only). 0\n",
            "    means that\n",
            "    the data\n",
            "    will be\n",
            "    loaded in\n",
            "    the main\n",
            "    process.\n",
            "    (default:\n",
            "    0)\n",
            "  --past_index PAST_INDEX\n",
            "    If >=0,\n",
            "    uses the co\n",
            "    rresponding\n",
            "    part of the\n",
            "    output as\n",
            "    the past\n",
            "    state for\n",
            "    next step.\n",
            "    (default:\n",
            "    -1)\n",
            "  --run_name RUN_NAME\n",
            "    An optional\n",
            "    descriptor\n",
            "    for the\n",
            "    run.\n",
            "    Notably\n",
            "    used for\n",
            "    wandb\n",
            "    logging.\n",
            "    (default:\n",
            "    None)\n",
            "  --disable_tqdm DISABLE_TQDM\n",
            "    Whether or\n",
            "    not to\n",
            "    disable the\n",
            "    tqdm\n",
            "    progress\n",
            "    bars.\n",
            "    (default:\n",
            "    None)\n",
            "  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\n",
            "    Remove\n",
            "    columns not\n",
            "    required by\n",
            "    the model\n",
            "    when using\n",
            "    an nlp.Data\n",
            "    set.\n",
            "    (default:\n",
            "    True)\n",
            "  --no_remove_unused_columns\n",
            "    Remove\n",
            "    columns not\n",
            "    required by\n",
            "    the model\n",
            "    when using\n",
            "    an nlp.Data\n",
            "    set.\n",
            "    (default:\n",
            "    False)\n",
            "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
            "    The list of\n",
            "    keys in\n",
            "    your\n",
            "    dictionary\n",
            "    of inputs\n",
            "    that\n",
            "    correspond\n",
            "    to the\n",
            "    labels.\n",
            "    (default:\n",
            "    None)\n",
            "  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\n",
            "    Whether or\n",
            "    not to load\n",
            "    the best\n",
            "    model found\n",
            "    during\n",
            "    training at\n",
            "    the end of\n",
            "    training.\n",
            "    When this\n",
            "    option is\n",
            "    enabled,\n",
            "    the best\n",
            "    checkpoint\n",
            "    will always\n",
            "    be saved.\n",
            "    See `save_t\n",
            "    otal_limit`\n",
            "    for more.\n",
            "    (default:\n",
            "    False)\n",
            "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
            "    The metric\n",
            "    to use to\n",
            "    compare two\n",
            "    different\n",
            "    models.\n",
            "    (default:\n",
            "    None)\n",
            "  --greater_is_better GREATER_IS_BETTER\n",
            "    Whether the\n",
            "    `metric_for\n",
            "    _best_model\n",
            "    ` should be\n",
            "    maximized\n",
            "    or not.\n",
            "    (default:\n",
            "    None)\n",
            "  --ignore_data_skip [IGNORE_DATA_SKIP]\n",
            "    When\n",
            "    resuming\n",
            "    training,\n",
            "    whether or\n",
            "    not to skip\n",
            "    the first\n",
            "    epochs and\n",
            "    batches to\n",
            "    get to the\n",
            "    same\n",
            "    training\n",
            "    data.\n",
            "    (default:\n",
            "    False)\n",
            "  --sharded_ddp SHARDED_DDP\n",
            "    Whether or\n",
            "    not to use\n",
            "    sharded DDP\n",
            "    training\n",
            "    (in\n",
            "    distributed\n",
            "    training\n",
            "    only). The\n",
            "    base option\n",
            "    should be\n",
            "    `simple`,\n",
            "    `zero_dp_2`\n",
            "    or\n",
            "    `zero_dp_3`\n",
            "    and you can\n",
            "    add CPU-\n",
            "    offload to\n",
            "    `zero_dp_2`\n",
            "    or\n",
            "    `zero_dp_3`\n",
            "    like this:\n",
            "    zero_dp_2\n",
            "    offload` or\n",
            "    `zero_dp_3\n",
            "    offload`.\n",
            "    You can add\n",
            "    auto-wrap\n",
            "    to\n",
            "    `zero_dp_2`\n",
            "    or\n",
            "    `zero_dp_3`\n",
            "    with the\n",
            "    same\n",
            "    syntax:\n",
            "    zero_dp_2\n",
            "    auto_wrap`\n",
            "    or\n",
            "    `zero_dp_3\n",
            "    auto_wrap`.\n",
            "    (default: )\n",
            "  --fsdp FSDP\n",
            "    Whether or\n",
            "    not to use\n",
            "    PyTorch\n",
            "    Fully\n",
            "    Sharded\n",
            "    Data\n",
            "    Parallel\n",
            "    (FSDP)\n",
            "    training\n",
            "    (in\n",
            "    distributed\n",
            "    training\n",
            "    only). The\n",
            "    base option\n",
            "    should be `\n",
            "    full_shard`\n",
            "    , `shard_gr\n",
            "    ad_op` or\n",
            "    `no_shard`\n",
            "    and you can\n",
            "    add CPU-\n",
            "    offload to \n",
            "    `full_shard\n",
            "    ` or `shard\n",
            "    _grad_op`\n",
            "    like this:\n",
            "    full_shard\n",
            "    offload` or\n",
            "    `shard_grad\n",
            "    _op\n",
            "    offload`.\n",
            "    You can add\n",
            "    auto-wrap\n",
            "    to `full_sh\n",
            "    ard` or `sh\n",
            "    ard_grad_op\n",
            "    ` with the\n",
            "    same\n",
            "    syntax:\n",
            "    full_shard\n",
            "    auto_wrap`\n",
            "    or `shard_g\n",
            "    rad_op\n",
            "    auto_wrap`.\n",
            "    (default: )\n",
            "  --fsdp_min_num_params FSDP_MIN_NUM_PARAMS\n",
            "    This\n",
            "    parameter\n",
            "    is\n",
            "    deprecated.\n",
            "    FSDP's\n",
            "    minimum\n",
            "    number of\n",
            "    parameters\n",
            "    for Default\n",
            "    Auto\n",
            "    Wrapping.\n",
            "    (useful\n",
            "    only when\n",
            "    `fsdp`\n",
            "    field is\n",
            "    passed).\n",
            "    (default:\n",
            "    0)\n",
            "  --fsdp_config FSDP_CONFIG\n",
            "    Config to\n",
            "    be used\n",
            "    with FSDP\n",
            "    (Pytorch\n",
            "    Fully\n",
            "    Sharded\n",
            "    Data\n",
            "    Parallel).\n",
            "    The value\n",
            "    is either\n",
            "    afsdp json\n",
            "    config file\n",
            "    (e.g., `fsd\n",
            "    p_config.js\n",
            "    on`) or an\n",
            "    already\n",
            "    loaded json\n",
            "    file as\n",
            "    `dict`.\n",
            "    (default:\n",
            "    None)\n",
            "  --fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP\n",
            "    This\n",
            "    parameter\n",
            "    is\n",
            "    deprecated.\n",
            "    Transformer\n",
            "    layer class\n",
            "    name (case-\n",
            "    sensitive)\n",
            "    to wrap,\n",
            "    e.g, `BertL\n",
            "    ayer`, `GPT\n",
            "    JBlock`,\n",
            "    `T5Block`\n",
            "    ....\n",
            "    (useful\n",
            "    only when\n",
            "    `fsdp` flag\n",
            "    is passed).\n",
            "    (default:\n",
            "    None)\n",
            "  --deepspeed DEEPSPEED\n",
            "    Enable\n",
            "    deepspeed\n",
            "    and pass\n",
            "    the path to\n",
            "    deepspeed\n",
            "    json config\n",
            "    file (e.g. \n",
            "    ds_config.j\n",
            "    son) or an\n",
            "    already\n",
            "    loaded json\n",
            "    file as a\n",
            "    dict\n",
            "    (default:\n",
            "    None)\n",
            "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
            "    The label\n",
            "    smoothing\n",
            "    epsilon to\n",
            "    apply (zero\n",
            "    means no\n",
            "    label\n",
            "    smoothing).\n",
            "    (default:\n",
            "    0.0)\n",
            "  --optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit}\n",
            "    The\n",
            "    optimizer\n",
            "    to use.\n",
            "    (default:\n",
            "    adamw_hf)\n",
            "  --optim_args OPTIM_ARGS\n",
            "    Optional\n",
            "    arguments\n",
            "    to supply\n",
            "    to\n",
            "    optimizer.\n",
            "    (default:\n",
            "    None)\n",
            "  --adafactor [ADAFACTOR]\n",
            "    Whether or\n",
            "    not to\n",
            "    replace\n",
            "    AdamW by\n",
            "    Adafactor.\n",
            "    (default:\n",
            "    False)\n",
            "  --group_by_length [GROUP_BY_LENGTH]\n",
            "    Whether or\n",
            "    not to\n",
            "    group\n",
            "    samples of\n",
            "    roughly the\n",
            "    same length\n",
            "    together\n",
            "    when\n",
            "    batching.\n",
            "    (default:\n",
            "    False)\n",
            "  --length_column_name LENGTH_COLUMN_NAME\n",
            "    Column name\n",
            "    with\n",
            "    precomputed\n",
            "    lengths to\n",
            "    use when\n",
            "    grouping by\n",
            "    length.\n",
            "    (default:\n",
            "    length)\n",
            "  --report_to REPORT_TO [REPORT_TO ...]\n",
            "    The list of\n",
            "    integration\n",
            "    s to report\n",
            "    the results\n",
            "    and logs\n",
            "    to.\n",
            "    (default:\n",
            "    None)\n",
            "  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\n",
            "    When using\n",
            "    distributed\n",
            "    training,\n",
            "    the value\n",
            "    of the flag\n",
            "    `find_unuse\n",
            "    d_parameter\n",
            "    s` passed\n",
            "    to `Distrib\n",
            "    utedDataPar\n",
            "    allel`.\n",
            "    (default:\n",
            "    None)\n",
            "  --ddp_bucket_cap_mb DDP_BUCKET_CAP_MB\n",
            "    When using\n",
            "    distributed\n",
            "    training,\n",
            "    the value\n",
            "    of the flag\n",
            "    `bucket_cap\n",
            "    _mb` passed\n",
            "    to `Distrib\n",
            "    utedDataPar\n",
            "    allel`.\n",
            "    (default:\n",
            "    None)\n",
            "  --ddp_broadcast_buffers DDP_BROADCAST_BUFFERS\n",
            "    When using\n",
            "    distributed\n",
            "    training,\n",
            "    the value\n",
            "    of the flag\n",
            "    `broadcast_\n",
            "    buffers`\n",
            "    passed to `\n",
            "    Distributed\n",
            "    DataParalle\n",
            "    l`.\n",
            "    (default:\n",
            "    None)\n",
            "  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\n",
            "    Whether or\n",
            "    not to pin\n",
            "    memory for\n",
            "    DataLoader.\n",
            "    (default:\n",
            "    True)\n",
            "  --no_dataloader_pin_memory\n",
            "    Whether or\n",
            "    not to pin\n",
            "    memory for\n",
            "    DataLoader.\n",
            "    (default:\n",
            "    False)\n",
            "  --skip_memory_metrics [SKIP_MEMORY_METRICS]\n",
            "    Whether or\n",
            "    not to skip\n",
            "    adding of\n",
            "    memory\n",
            "    profiler\n",
            "    reports to\n",
            "    metrics.\n",
            "    (default:\n",
            "    True)\n",
            "  --no_skip_memory_metrics\n",
            "    Whether or\n",
            "    not to skip\n",
            "    adding of\n",
            "    memory\n",
            "    profiler\n",
            "    reports to\n",
            "    metrics.\n",
            "    (default:\n",
            "    False)\n",
            "  --use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]\n",
            "    Whether or\n",
            "    not to use\n",
            "    the legacy \n",
            "    prediction_\n",
            "    loop in the\n",
            "    Trainer.\n",
            "    (default:\n",
            "    False)\n",
            "  --push_to_hub [PUSH_TO_HUB]\n",
            "    Whether or\n",
            "    not to\n",
            "    upload the\n",
            "    trained\n",
            "    model to\n",
            "    the model\n",
            "    hub after\n",
            "    training.\n",
            "    (default:\n",
            "    False)\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "    The path to\n",
            "    a folder\n",
            "    with a\n",
            "    valid\n",
            "    checkpoint\n",
            "    for your\n",
            "    model.\n",
            "    (default:\n",
            "    None)\n",
            "  --hub_model_id HUB_MODEL_ID\n",
            "    The name of\n",
            "    the\n",
            "    repository\n",
            "    to keep in\n",
            "    sync with\n",
            "    the local `\n",
            "    output_dir`\n",
            "    . (default:\n",
            "    None)\n",
            "  --hub_strategy {end,every_save,checkpoint,all_checkpoints}\n",
            "    The hub\n",
            "    strategy to\n",
            "    use when `-\n",
            "    -push_to_hu\n",
            "    b` is\n",
            "    activated.\n",
            "    (default:\n",
            "    every_save)\n",
            "  --hub_token HUB_TOKEN\n",
            "    The token\n",
            "    to use to\n",
            "    push to the\n",
            "    Model Hub.\n",
            "    (default:\n",
            "    None)\n",
            "  --hub_private_repo [HUB_PRIVATE_REPO]\n",
            "    Whether the\n",
            "    model\n",
            "    repository\n",
            "    is private\n",
            "    or not.\n",
            "    (default:\n",
            "    False)\n",
            "  --gradient_checkpointing [GRADIENT_CHECKPOINTING]\n",
            "    If True,\n",
            "    use\n",
            "    gradient ch\n",
            "    eckpointing\n",
            "    to save\n",
            "    memory at\n",
            "    the expense\n",
            "    of slower\n",
            "    backward\n",
            "    pass.\n",
            "    (default:\n",
            "    False)\n",
            "  --include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]\n",
            "    Whether or\n",
            "    not the\n",
            "    inputs will\n",
            "    be passed\n",
            "    to the `com\n",
            "    pute_metric\n",
            "    s`\n",
            "    function.\n",
            "    (default:\n",
            "    False)\n",
            "  --fp16_backend {auto,cuda_amp,apex,cpu_amp}\n",
            "    Deprecated.\n",
            "    Use half_pr\n",
            "    ecision_bac\n",
            "    kend\n",
            "    instead\n",
            "    (default:\n",
            "    auto)\n",
            "  --push_to_hub_model_id PUSH_TO_HUB_MODEL_ID\n",
            "    The name of\n",
            "    the\n",
            "    repository\n",
            "    to which\n",
            "    push the\n",
            "    `Trainer`.\n",
            "    (default:\n",
            "    None)\n",
            "  --push_to_hub_organization PUSH_TO_HUB_ORGANIZATION\n",
            "    The name of\n",
            "    the organiz\n",
            "    ation in\n",
            "    with to\n",
            "    which push\n",
            "    the\n",
            "    `Trainer`.\n",
            "    (default:\n",
            "    None)\n",
            "  --push_to_hub_token PUSH_TO_HUB_TOKEN\n",
            "    The token\n",
            "    to use to\n",
            "    push to the\n",
            "    Model Hub.\n",
            "    (default:\n",
            "    None)\n",
            "  --mp_parameters MP_PARAMETERS\n",
            "    Used by the\n",
            "    SageMaker\n",
            "    launcher to\n",
            "    send mp-\n",
            "    specific\n",
            "    args.\n",
            "    Ignored in\n",
            "    Trainer\n",
            "    (default: )\n",
            "  --auto_find_batch_size [AUTO_FIND_BATCH_SIZE]\n",
            "    Whether to \n",
            "    automatical\n",
            "    ly decrease\n",
            "    the batch\n",
            "    size in\n",
            "    half and\n",
            "    rerun the\n",
            "    training\n",
            "    loop again\n",
            "    each time a\n",
            "    CUDA Out-\n",
            "    of-Memory\n",
            "    was reached\n",
            "    (default:\n",
            "    False)\n",
            "  --full_determinism [FULL_DETERMINISM]\n",
            "    Whether to\n",
            "    call enable\n",
            "    _full_deter\n",
            "    minism\n",
            "    instead of\n",
            "    set_seed\n",
            "    for reprodu\n",
            "    cibility in\n",
            "    distributed\n",
            "    training.\n",
            "    Important:\n",
            "    this will\n",
            "    negatively\n",
            "    impact the \n",
            "    performance\n",
            "    , so only\n",
            "    use it for\n",
            "    debugging.\n",
            "    (default:\n",
            "    False)\n",
            "  --torchdynamo TORCHDYNAMO\n",
            "    This\n",
            "    argument is\n",
            "    deprecated,\n",
            "    use `--\n",
            "    torch_compi\n",
            "    le_backend`\n",
            "    instead.\n",
            "    (default:\n",
            "    None)\n",
            "  --ray_scope RAY_SCOPE\n",
            "    The scope\n",
            "    to use when\n",
            "    doing hyper\n",
            "    parameter\n",
            "    search with\n",
            "    Ray. By\n",
            "    default,\n",
            "    `\"last\"`\n",
            "    will be\n",
            "    used. Ray\n",
            "    will then\n",
            "    use the\n",
            "    last\n",
            "    checkpoint\n",
            "    of all\n",
            "    trials,\n",
            "    compare\n",
            "    those, and\n",
            "    select the\n",
            "    best one.\n",
            "    However,\n",
            "    other\n",
            "    options are\n",
            "    also\n",
            "    available.\n",
            "    See the Ray\n",
            "    documentati\n",
            "    on (https:/\n",
            "    /docs.ray.i\n",
            "    o/en/latest\n",
            "    /tune/api_d\n",
            "    ocs/analysi\n",
            "    s.html#ray.\n",
            "    tune.Experi\n",
            "    mentAnalysi\n",
            "    s.get_best_\n",
            "    trial) for\n",
            "    more\n",
            "    options.\n",
            "    (default:\n",
            "    last)\n",
            "  --ddp_timeout DDP_TIMEOUT\n",
            "    Overrides\n",
            "    the default\n",
            "    timeout for\n",
            "    distributed\n",
            "    training\n",
            "    (value\n",
            "    should be\n",
            "    given in\n",
            "    seconds).\n",
            "    (default:\n",
            "    1800)\n",
            "  --torch_compile [TORCH_COMPILE]\n",
            "    If set to\n",
            "    `True`, the\n",
            "    model will\n",
            "    be wrapped\n",
            "    in `torch.c\n",
            "    ompile`.\n",
            "    (default:\n",
            "    False)\n",
            "  --torch_compile_backend TORCH_COMPILE_BACKEND\n",
            "    Which\n",
            "    backend to\n",
            "    use with `t\n",
            "    orch.compil\n",
            "    e`, passing\n",
            "    one will\n",
            "    trigger a\n",
            "    model compi\n",
            "    lation.\n",
            "    (default:\n",
            "    None)\n",
            "  --torch_compile_mode TORCH_COMPILE_MODE\n",
            "    Which mode\n",
            "    to use with\n",
            "    `torch.comp\n",
            "    ile`,\n",
            "    passing one\n",
            "    will\n",
            "    trigger a\n",
            "    model compi\n",
            "    lation.\n",
            "    (default:\n",
            "    None)\n",
            "  --xpu_backend {mpi,ccl,gloo}\n",
            "    The backend\n",
            "    to be used\n",
            "    for\n",
            "    distributed\n",
            "    training on\n",
            "    Intel XPU.\n",
            "    (default:\n",
            "    None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4LOysJ75QEt",
        "outputId": "b249a472-9c9d-46ec-fa59-c325280389d4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python banglabert/sequence_classification/sequence_classification.py \\\n",
        "    --model_name_or_path \"csebuetnlp/banglabert\" \\\n",
        "    --dataset_dir \"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/dataset/Tariq/split/\" \\\n",
        "    --output_dir \"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/models/banglabert/\" \\\n",
        "    --learning_rate=2e-5 \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --gradient_accumulation_steps 2 \\\n",
        "    --weight_decay 0.1 \\\n",
        "    --lr_scheduler_type \"linear\"  \\\n",
        "    --per_device_train_batch_size=8 \\\n",
        "    --per_device_eval_batch_size=8 \\\n",
        "    --max_seq_length 512 \\\n",
        "    --logging_strategy \"epoch\" \\\n",
        "    --evaluation_strategy \"epoch\" \\\n",
        "    --num_train_epochs=10 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train --do_eval --do_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bfDpvE8BoeW",
        "outputId": "6f403fea-f658-4a3c-af1f-78713dc41a9e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-15 04:29:28.785392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/15/2023 04:29:31 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 2089.14it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00,  4.84it/s]\n",
            "Generating train split: 2700 examples [00:00, 82913.84 examples/s]\n",
            "Generating validation split: 1330 examples [00:00, 80252.40 examples/s]\n",
            "Generating test split: 2016 examples [00:00, 69649.41 examples/s]\n",
            "[WARNING|modeling_utils.py:3331] 2023-08-15 04:29:34,212 >> Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running normalization on dataset: 100% 2700/2700 [00:00<00:00, 3400.78 examples/s]\n",
            "Running normalization on dataset: 100% 1330/1330 [00:00<00:00, 3284.42 examples/s]\n",
            "Running normalization on dataset: 100% 2016/2016 [00:00<00:00, 3167.63 examples/s]\n",
            "Running tokenizer on dataset: 100% 2700/2700 [00:01<00:00, 2680.41 examples/s]\n",
            "Running tokenizer on dataset: 100% 1330/1330 [00:00<00:00, 2381.39 examples/s]\n",
            "Running tokenizer on dataset: 100% 2016/2016 [00:01<00:00, 1224.25 examples/s]\n",
            "/content/banglabert/sequence_classification/sequence_classification.py:380: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  required_metrics = [load_metric(k) for k in metric_names]\n",
            "Downloading builder script: 4.21kB [00:00, 8.49MB/s]       \n",
            "Downloading builder script: 7.55kB [00:00, 12.3MB/s]       \n",
            "Downloading builder script: 7.38kB [00:00, 13.0MB/s]       \n",
            "Downloading builder script: 6.50kB [00:00, 11.3MB/s]       \n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "{'loss': 0.9404, 'learning_rate': 2e-05, 'epoch': 1.0}\n",
            " 10% 169/1690 [00:35<04:14,  5.98it/s]\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 5/167 [00:00<00:03, 41.22it/s]\u001b[A\n",
            "  6% 10/167 [00:00<00:04, 33.74it/s]\u001b[A\n",
            "  8% 14/167 [00:00<00:04, 35.16it/s]\u001b[A\n",
            " 11% 18/167 [00:00<00:04, 34.25it/s]\u001b[A\n",
            " 13% 22/167 [00:00<00:04, 33.32it/s]\u001b[A\n",
            " 16% 26/167 [00:00<00:04, 31.26it/s]\u001b[A\n",
            " 18% 30/167 [00:00<00:04, 32.69it/s]\u001b[A\n",
            " 20% 34/167 [00:01<00:03, 34.39it/s]\u001b[A\n",
            " 23% 38/167 [00:01<00:03, 34.43it/s]\u001b[A\n",
            " 25% 42/167 [00:01<00:03, 33.37it/s]\u001b[A\n",
            " 28% 47/167 [00:01<00:03, 36.19it/s]\u001b[A\n",
            " 31% 51/167 [00:01<00:03, 35.90it/s]\u001b[A\n",
            " 33% 55/167 [00:01<00:03, 32.50it/s]\u001b[A\n",
            " 35% 59/167 [00:01<00:03, 33.73it/s]\u001b[A\n",
            " 38% 63/167 [00:01<00:02, 34.89it/s]\u001b[A\n",
            " 40% 67/167 [00:01<00:02, 35.16it/s]\u001b[A\n",
            " 43% 71/167 [00:02<00:02, 33.74it/s]\u001b[A\n",
            " 45% 75/167 [00:02<00:02, 34.10it/s]\u001b[A\n",
            " 47% 79/167 [00:02<00:02, 33.75it/s]\u001b[A\n",
            " 50% 83/167 [00:02<00:02, 35.30it/s]\u001b[A\n",
            " 52% 87/167 [00:02<00:02, 34.35it/s]\u001b[A\n",
            " 54% 91/167 [00:02<00:02, 35.21it/s]\u001b[A\n",
            " 57% 95/167 [00:02<00:02, 35.71it/s]\u001b[A\n",
            " 59% 99/167 [00:02<00:01, 36.17it/s]\u001b[A\n",
            " 62% 104/167 [00:02<00:01, 37.84it/s]\u001b[A\n",
            " 65% 108/167 [00:03<00:01, 35.67it/s]\u001b[A\n",
            " 67% 112/167 [00:03<00:01, 35.30it/s]\u001b[A\n",
            " 70% 117/167 [00:03<00:01, 38.57it/s]\u001b[A\n",
            " 72% 121/167 [00:03<00:01, 38.34it/s]\u001b[A\n",
            " 75% 125/167 [00:03<00:01, 38.06it/s]\u001b[A\n",
            " 78% 130/167 [00:03<00:00, 39.55it/s]\u001b[A\n",
            " 81% 135/167 [00:03<00:00, 41.28it/s]\u001b[A\n",
            " 84% 140/167 [00:03<00:00, 43.04it/s]\u001b[A\n",
            " 87% 145/167 [00:03<00:00, 43.21it/s]\u001b[A\n",
            " 90% 150/167 [00:04<00:00, 41.02it/s]\u001b[A\n",
            " 93% 155/167 [00:04<00:00, 42.18it/s]\u001b[A\n",
            " 96% 160/167 [00:04<00:00, 42.98it/s]\u001b[A\n",
            " 99% 165/167 [00:04<00:00, 41.16it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7320082187652588, 'eval_accuracy': 0.693984962406015, 'eval_precision': 0.46431915545943436, 'eval_recall': 0.5036874266277355, 'eval_f1': 0.4721731067595729, 'eval_runtime': 4.6615, 'eval_samples_per_second': 285.315, 'eval_steps_per_second': 35.825, 'epoch': 1.0}\n",
            " 10% 169/1690 [00:40<04:14,  5.98it/s]\n",
            "100% 167/167 [00:04<00:00, 41.16it/s]\u001b[A\n",
            "{'loss': 0.5737, 'learning_rate': 1.7777777777777777e-05, 'epoch': 2.0}\n",
            " 20% 338/1690 [01:13<04:13,  5.32it/s]\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 5/167 [00:00<00:04, 36.25it/s]\u001b[A\n",
            "  5% 9/167 [00:00<00:05, 30.13it/s]\u001b[A\n",
            "  8% 13/167 [00:00<00:04, 32.77it/s]\u001b[A\n",
            " 10% 17/167 [00:00<00:04, 30.38it/s]\u001b[A\n",
            " 13% 21/167 [00:00<00:04, 30.12it/s]\u001b[A\n",
            " 15% 25/167 [00:00<00:04, 30.19it/s]\u001b[A\n",
            " 17% 29/167 [00:00<00:04, 31.15it/s]\u001b[A\n",
            " 20% 33/167 [00:01<00:04, 31.13it/s]\u001b[A\n",
            " 22% 37/167 [00:01<00:04, 32.09it/s]\u001b[A\n",
            " 25% 41/167 [00:01<00:03, 34.16it/s]\u001b[A\n",
            " 27% 45/167 [00:01<00:03, 34.00it/s]\u001b[A\n",
            " 29% 49/167 [00:01<00:03, 35.29it/s]\u001b[A\n",
            " 32% 53/167 [00:01<00:03, 34.44it/s]\u001b[A\n",
            " 34% 57/167 [00:01<00:03, 32.99it/s]\u001b[A\n",
            " 37% 61/167 [00:01<00:03, 33.19it/s]\u001b[A\n",
            " 39% 65/167 [00:01<00:03, 32.81it/s]\u001b[A\n",
            " 41% 69/167 [00:02<00:02, 33.75it/s]\u001b[A\n",
            " 44% 73/167 [00:02<00:02, 32.34it/s]\u001b[A\n",
            " 46% 77/167 [00:02<00:02, 32.98it/s]\u001b[A\n",
            " 49% 81/167 [00:02<00:02, 33.80it/s]\u001b[A\n",
            " 51% 85/167 [00:02<00:02, 34.83it/s]\u001b[A\n",
            " 53% 89/167 [00:02<00:02, 35.14it/s]\u001b[A\n",
            " 56% 93/167 [00:02<00:02, 35.55it/s]\u001b[A\n",
            " 58% 97/167 [00:02<00:02, 34.75it/s]\u001b[A\n",
            " 61% 102/167 [00:03<00:01, 38.21it/s]\u001b[A\n",
            " 63% 106/167 [00:03<00:01, 36.77it/s]\u001b[A\n",
            " 66% 110/167 [00:03<00:01, 36.91it/s]\u001b[A\n",
            " 68% 114/167 [00:03<00:01, 36.81it/s]\u001b[A\n",
            " 71% 119/167 [00:03<00:01, 40.26it/s]\u001b[A\n",
            " 74% 124/167 [00:03<00:01, 38.35it/s]\u001b[A\n",
            " 77% 129/167 [00:03<00:00, 41.05it/s]\u001b[A\n",
            " 80% 134/167 [00:03<00:00, 41.91it/s]\u001b[A\n",
            " 83% 139/167 [00:03<00:00, 43.80it/s]\u001b[A\n",
            " 86% 144/167 [00:04<00:00, 43.81it/s]\u001b[A\n",
            " 89% 149/167 [00:04<00:00, 41.80it/s]\u001b[A\n",
            " 92% 154/167 [00:04<00:00, 42.23it/s]\u001b[A\n",
            " 95% 159/167 [00:04<00:00, 43.29it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.49947845935821533, 'eval_accuracy': 0.8112781954887218, 'eval_precision': 0.8077121733641554, 'eval_recall': 0.7784211875653605, 'eval_f1': 0.7913278963356087, 'eval_runtime': 4.6824, 'eval_samples_per_second': 284.043, 'eval_steps_per_second': 35.666, 'epoch': 2.0}\n",
            " 20% 338/1690 [01:18<04:13,  5.32it/s]\n",
            "100% 167/167 [00:04<00:00, 41.53it/s]\u001b[A\n",
            "{'loss': 0.3487, 'learning_rate': 1.555555555555556e-05, 'epoch': 3.0}\n",
            " 30% 507/1690 [02:03<12:57,  1.52it/s]\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 4/167 [00:00<00:04, 33.99it/s]\u001b[A\n",
            "  5% 8/167 [00:00<00:05, 26.72it/s]\u001b[A\n",
            "  7% 11/167 [00:00<00:05, 26.31it/s]\u001b[A\n",
            "  8% 14/167 [00:00<00:05, 25.69it/s]\u001b[A\n",
            " 10% 17/167 [00:00<00:05, 25.63it/s]\u001b[A\n",
            " 12% 20/167 [00:00<00:05, 25.68it/s]\u001b[A\n",
            " 14% 23/167 [00:00<00:05, 24.94it/s]\u001b[A\n",
            " 16% 26/167 [00:01<00:05, 25.46it/s]\u001b[A\n",
            " 18% 30/167 [00:01<00:05, 27.07it/s]\u001b[A\n",
            " 20% 33/167 [00:01<00:05, 26.52it/s]\u001b[A\n",
            " 22% 36/167 [00:01<00:05, 25.79it/s]\u001b[A\n",
            " 23% 39/167 [00:01<00:05, 25.53it/s]\u001b[A\n",
            " 25% 42/167 [00:01<00:04, 25.01it/s]\u001b[A\n",
            " 27% 45/167 [00:01<00:04, 25.50it/s]\u001b[A\n",
            " 29% 48/167 [00:01<00:04, 24.16it/s]\u001b[A\n",
            " 31% 51/167 [00:01<00:04, 24.58it/s]\u001b[A\n",
            " 32% 54/167 [00:02<00:04, 23.22it/s]\u001b[A\n",
            " 34% 57/167 [00:02<00:04, 22.92it/s]\u001b[A\n",
            " 36% 60/167 [00:02<00:04, 24.19it/s]\u001b[A\n",
            " 38% 63/167 [00:02<00:04, 23.05it/s]\u001b[A\n",
            " 40% 66/167 [00:02<00:04, 22.89it/s]\u001b[A\n",
            " 41% 69/167 [00:02<00:04, 23.16it/s]\u001b[A\n",
            " 43% 72/167 [00:02<00:04, 23.36it/s]\u001b[A\n",
            " 45% 75/167 [00:03<00:03, 24.40it/s]\u001b[A\n",
            " 47% 78/167 [00:03<00:03, 24.78it/s]\u001b[A\n",
            " 49% 81/167 [00:03<00:03, 25.63it/s]\u001b[A\n",
            " 50% 84/167 [00:03<00:03, 25.71it/s]\u001b[A\n",
            " 52% 87/167 [00:03<00:03, 24.39it/s]\u001b[A\n",
            " 54% 90/167 [00:03<00:03, 21.45it/s]\u001b[A\n",
            " 56% 93/167 [00:03<00:03, 22.00it/s]\u001b[A\n",
            " 57% 96/167 [00:03<00:03, 21.83it/s]\u001b[A\n",
            " 59% 99/167 [00:04<00:03, 21.77it/s]\u001b[A\n",
            " 61% 102/167 [00:04<00:02, 21.96it/s]\u001b[A\n",
            " 63% 105/167 [00:04<00:02, 21.72it/s]\u001b[A\n",
            " 65% 108/167 [00:04<00:02, 22.45it/s]\u001b[A\n",
            " 66% 111/167 [00:04<00:02, 24.21it/s]\u001b[A\n",
            " 68% 114/167 [00:04<00:02, 23.76it/s]\u001b[A\n",
            " 71% 118/167 [00:04<00:01, 27.71it/s]\u001b[A\n",
            " 73% 122/167 [00:04<00:01, 29.12it/s]\u001b[A\n",
            " 75% 125/167 [00:05<00:01, 28.78it/s]\u001b[A\n",
            " 78% 130/167 [00:05<00:01, 31.58it/s]\u001b[A\n",
            " 80% 134/167 [00:05<00:01, 31.38it/s]\u001b[A\n",
            " 83% 139/167 [00:05<00:00, 34.17it/s]\u001b[A\n",
            " 86% 143/167 [00:05<00:00, 33.22it/s]\u001b[A\n",
            " 89% 148/167 [00:05<00:00, 34.54it/s]\u001b[A\n",
            " 91% 152/167 [00:05<00:00, 35.45it/s]\u001b[A\n",
            " 94% 157/167 [00:05<00:00, 37.15it/s]\u001b[A\n",
            " 96% 161/167 [00:06<00:00, 37.17it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.595315158367157, 'eval_accuracy': 0.8097744360902256, 'eval_precision': 0.8343266720575429, 'eval_recall': 0.7474416504709306, 'eval_f1': 0.7743797772007229, 'eval_runtime': 6.3439, 'eval_samples_per_second': 209.65, 'eval_steps_per_second': 26.324, 'epoch': 3.0}\n",
            " 30% 507/1690 [02:09<12:57,  1.52it/s]\n",
            "100% 167/167 [00:06<00:00, 35.18it/s]\u001b[A\n",
            "{'loss': 0.2058, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}\n",
            " 40% 676/1690 [02:45<02:55,  5.76it/s]\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 4/167 [00:00<00:04, 38.85it/s]\u001b[A\n",
            "  5% 8/167 [00:00<00:05, 28.31it/s]\u001b[A\n",
            "  7% 12/167 [00:00<00:04, 32.31it/s]\u001b[A\n",
            " 10% 16/167 [00:00<00:04, 30.27it/s]\u001b[A\n",
            " 12% 20/167 [00:00<00:04, 29.41it/s]\u001b[A\n",
            " 14% 24/167 [00:00<00:04, 28.70it/s]\u001b[A\n",
            " 17% 28/167 [00:00<00:04, 29.82it/s]\u001b[A\n",
            " 19% 32/167 [00:01<00:04, 30.44it/s]\u001b[A\n",
            " 22% 36/167 [00:01<00:04, 31.60it/s]\u001b[A\n",
            " 24% 40/167 [00:01<00:03, 32.55it/s]\u001b[A\n",
            " 26% 44/167 [00:01<00:03, 31.50it/s]\u001b[A\n",
            " 29% 49/167 [00:01<00:03, 33.95it/s]\u001b[A\n",
            " 32% 53/167 [00:01<00:03, 32.95it/s]\u001b[A\n",
            " 34% 57/167 [00:01<00:03, 30.84it/s]\u001b[A\n",
            " 37% 61/167 [00:01<00:03, 31.17it/s]\u001b[A\n",
            " 39% 65/167 [00:02<00:03, 31.59it/s]\u001b[A\n",
            " 41% 69/167 [00:02<00:02, 32.67it/s]\u001b[A\n",
            " 44% 73/167 [00:02<00:03, 30.85it/s]\u001b[A\n",
            " 46% 77/167 [00:02<00:02, 31.81it/s]\u001b[A\n",
            " 49% 81/167 [00:02<00:02, 32.61it/s]\u001b[A\n",
            " 51% 85/167 [00:02<00:02, 33.35it/s]\u001b[A\n",
            " 53% 89/167 [00:02<00:02, 33.18it/s]\u001b[A\n",
            " 56% 93/167 [00:02<00:02, 32.89it/s]\u001b[A\n",
            " 58% 97/167 [00:03<00:02, 31.93it/s]\u001b[A\n",
            " 61% 102/167 [00:03<00:01, 35.22it/s]\u001b[A\n",
            " 63% 106/167 [00:03<00:01, 34.26it/s]\u001b[A\n",
            " 66% 110/167 [00:03<00:01, 34.55it/s]\u001b[A\n",
            " 68% 114/167 [00:03<00:01, 34.80it/s]\u001b[A\n",
            " 71% 119/167 [00:03<00:01, 37.89it/s]\u001b[A\n",
            " 74% 123/167 [00:03<00:01, 36.16it/s]\u001b[A\n",
            " 77% 128/167 [00:03<00:01, 38.25it/s]\u001b[A\n",
            " 79% 132/167 [00:03<00:00, 38.69it/s]\u001b[A\n",
            " 82% 137/167 [00:04<00:00, 40.06it/s]\u001b[A\n",
            " 85% 142/167 [00:04<00:00, 41.37it/s]\u001b[A\n",
            " 88% 147/167 [00:04<00:00, 42.19it/s]\u001b[A\n",
            " 91% 152/167 [00:04<00:00, 40.31it/s]\u001b[A\n",
            " 94% 157/167 [00:04<00:00, 41.07it/s]\u001b[A\n",
            " 97% 162/167 [00:04<00:00, 41.82it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.623363733291626, 'eval_accuracy': 0.8293233082706767, 'eval_precision': 0.8228985982365519, 'eval_recall': 0.8000318897814888, 'eval_f1': 0.8103835784279809, 'eval_runtime': 4.8963, 'eval_samples_per_second': 271.633, 'eval_steps_per_second': 34.107, 'epoch': 4.0}\n",
            " 40% 676/1690 [02:50<02:55,  5.76it/s]\n",
            "100% 167/167 [00:04<00:00, 40.99it/s]\u001b[A\n",
            "{'loss': 0.1334, 'learning_rate': 1.1111111111111113e-05, 'epoch': 5.0}\n",
            " 50% 845/1690 [03:25<02:50,  4.95it/s]\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 4/167 [00:00<00:04, 37.76it/s]\u001b[A\n",
            "  5% 8/167 [00:00<00:05, 29.22it/s]\u001b[A\n",
            "  7% 12/167 [00:00<00:04, 32.94it/s]\u001b[A\n",
            " 10% 16/167 [00:00<00:04, 30.63it/s]\u001b[A\n",
            " 12% 20/167 [00:00<00:04, 29.66it/s]\u001b[A\n",
            " 14% 24/167 [00:00<00:04, 29.34it/s]\u001b[A\n",
            " 17% 28/167 [00:00<00:04, 31.00it/s]\u001b[A\n",
            " 19% 32/167 [00:01<00:04, 30.82it/s]\u001b[A\n",
            " 22% 36/167 [00:01<00:04, 31.27it/s]\u001b[A\n",
            " 24% 40/167 [00:01<00:03, 32.20it/s]\u001b[A\n",
            " 26% 44/167 [00:01<00:03, 31.38it/s]\u001b[A\n",
            " 29% 49/167 [00:01<00:03, 32.62it/s]\u001b[A\n",
            " 32% 53/167 [00:01<00:03, 31.96it/s]\u001b[A\n",
            " 34% 57/167 [00:01<00:03, 30.65it/s]\u001b[A\n",
            " 37% 61/167 [00:01<00:03, 31.29it/s]\u001b[A\n",
            " 39% 65/167 [00:02<00:03, 31.39it/s]\u001b[A\n",
            " 41% 69/167 [00:02<00:03, 31.89it/s]\u001b[A\n",
            " 44% 73/167 [00:02<00:03, 30.26it/s]\u001b[A\n",
            " 46% 77/167 [00:02<00:02, 31.11it/s]\u001b[A\n",
            " 49% 81/167 [00:02<00:02, 31.95it/s]\u001b[A\n",
            " 51% 85/167 [00:02<00:02, 32.72it/s]\u001b[A\n",
            " 53% 89/167 [00:02<00:02, 33.06it/s]\u001b[A\n",
            " 56% 93/167 [00:02<00:02, 33.11it/s]\u001b[A\n",
            " 58% 97/167 [00:03<00:02, 31.97it/s]\u001b[A\n",
            " 61% 102/167 [00:03<00:01, 34.47it/s]\u001b[A\n",
            " 63% 106/167 [00:03<00:01, 33.80it/s]\u001b[A\n",
            " 66% 110/167 [00:03<00:01, 33.93it/s]\u001b[A\n",
            " 68% 114/167 [00:03<00:01, 33.91it/s]\u001b[A\n",
            " 71% 118/167 [00:03<00:01, 35.28it/s]\u001b[A\n",
            " 73% 122/167 [00:03<00:01, 34.86it/s]\u001b[A\n",
            " 75% 126/167 [00:03<00:01, 34.98it/s]\u001b[A\n",
            " 78% 130/167 [00:04<00:01, 35.10it/s]\u001b[A\n",
            " 81% 135/167 [00:04<00:00, 36.51it/s]\u001b[A\n",
            " 84% 140/167 [00:04<00:00, 37.88it/s]\u001b[A\n",
            " 87% 145/167 [00:04<00:00, 38.75it/s]\u001b[A\n",
            " 89% 149/167 [00:04<00:00, 37.37it/s]\u001b[A\n",
            " 92% 153/167 [00:04<00:00, 37.88it/s]\u001b[A\n",
            " 95% 158/167 [00:04<00:00, 39.31it/s]\u001b[A\n",
            " 97% 162/167 [00:04<00:00, 39.17it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7463430166244507, 'eval_accuracy': 0.8150375939849624, 'eval_precision': 0.811062807249248, 'eval_recall': 0.7892092610601127, 'eval_f1': 0.7971718761771153, 'eval_runtime': 5.07, 'eval_samples_per_second': 262.328, 'eval_steps_per_second': 32.939, 'epoch': 5.0}\n",
            " 50% 845/1690 [03:30<02:50,  4.95it/s]\n",
            "100% 167/167 [00:05<00:00, 37.81it/s]\u001b[A\n",
            "{'loss': 0.078, 'learning_rate': 8.888888888888888e-06, 'epoch': 6.0}\n",
            " 60% 1014/1690 [04:10<02:11,  5.14it/s]\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 5/167 [00:00<00:04, 36.07it/s]\u001b[A\n",
            "  5% 9/167 [00:00<00:05, 29.61it/s]\u001b[A\n",
            "  8% 14/167 [00:00<00:04, 32.49it/s]\u001b[A\n",
            " 11% 18/167 [00:00<00:04, 31.75it/s]\u001b[A\n",
            " 13% 22/167 [00:00<00:04, 30.88it/s]\u001b[A\n",
            " 16% 26/167 [00:00<00:04, 29.56it/s]\u001b[A\n",
            " 18% 30/167 [00:00<00:04, 30.88it/s]\u001b[A\n",
            " 20% 34/167 [00:01<00:04, 32.85it/s]\u001b[A\n",
            " 23% 38/167 [00:01<00:04, 31.77it/s]\u001b[A\n",
            " 25% 42/167 [00:01<00:04, 31.06it/s]\u001b[A\n",
            " 28% 46/167 [00:01<00:04, 28.49it/s]\u001b[A\n",
            " 29% 49/167 [00:01<00:04, 27.24it/s]\u001b[A\n",
            " 31% 52/167 [00:01<00:04, 24.82it/s]\u001b[A\n",
            " 33% 55/167 [00:01<00:04, 23.24it/s]\u001b[A\n",
            " 35% 58/167 [00:02<00:04, 22.99it/s]\u001b[A\n",
            " 37% 61/167 [00:02<00:04, 23.19it/s]\u001b[A\n",
            " 38% 64/167 [00:02<00:04, 23.13it/s]\u001b[A\n",
            " 40% 67/167 [00:02<00:04, 22.28it/s]\u001b[A\n",
            " 42% 70/167 [00:02<00:04, 21.32it/s]\u001b[A\n",
            " 44% 73/167 [00:02<00:04, 19.70it/s]\u001b[A\n",
            " 46% 76/167 [00:02<00:04, 20.82it/s]\u001b[A\n",
            " 47% 79/167 [00:03<00:04, 21.86it/s]\u001b[A\n",
            " 49% 82/167 [00:03<00:03, 23.16it/s]\u001b[A\n",
            " 51% 85/167 [00:03<00:03, 23.75it/s]\u001b[A\n",
            " 53% 88/167 [00:03<00:03, 24.31it/s]\u001b[A\n",
            " 54% 91/167 [00:03<00:03, 23.96it/s]\u001b[A\n",
            " 56% 94/167 [00:03<00:03, 23.40it/s]\u001b[A\n",
            " 58% 97/167 [00:03<00:03, 22.83it/s]\u001b[A\n",
            " 60% 100/167 [00:03<00:03, 22.16it/s]\u001b[A\n",
            " 62% 103/167 [00:04<00:02, 22.80it/s]\u001b[A\n",
            " 63% 106/167 [00:04<00:02, 22.49it/s]\u001b[A\n",
            " 65% 109/167 [00:04<00:02, 23.71it/s]\u001b[A\n",
            " 67% 112/167 [00:04<00:02, 22.88it/s]\u001b[A\n",
            " 69% 115/167 [00:04<00:02, 20.72it/s]\u001b[A\n",
            " 71% 118/167 [00:04<00:02, 18.37it/s]\u001b[A\n",
            " 72% 120/167 [00:04<00:02, 17.16it/s]\u001b[A\n",
            " 73% 122/167 [00:05<00:02, 16.76it/s]\u001b[A\n",
            " 74% 124/167 [00:05<00:02, 17.38it/s]\u001b[A\n",
            " 75% 126/167 [00:05<00:02, 17.33it/s]\u001b[A\n",
            " 77% 128/167 [00:05<00:02, 16.14it/s]\u001b[A\n",
            " 78% 130/167 [00:05<00:02, 14.25it/s]\u001b[A\n",
            " 79% 132/167 [00:05<00:02, 13.87it/s]\u001b[A\n",
            " 80% 134/167 [00:05<00:02, 13.25it/s]\u001b[A\n",
            " 82% 137/167 [00:06<00:01, 15.69it/s]\u001b[A\n",
            " 84% 140/167 [00:06<00:01, 17.63it/s]\u001b[A\n",
            " 86% 143/167 [00:06<00:01, 19.63it/s]\u001b[A\n",
            " 87% 146/167 [00:06<00:00, 21.59it/s]\u001b[A\n",
            " 89% 149/167 [00:06<00:00, 20.87it/s]\u001b[A\n",
            " 91% 152/167 [00:06<00:00, 19.74it/s]\u001b[A\n",
            " 93% 155/167 [00:06<00:00, 20.94it/s]\u001b[A\n",
            " 95% 158/167 [00:07<00:00, 22.09it/s]\u001b[A\n",
            " 96% 161/167 [00:07<00:00, 22.84it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8658689856529236, 'eval_accuracy': 0.818796992481203, 'eval_precision': 0.8293208349802543, 'eval_recall': 0.7771974718065185, 'eval_f1': 0.797721868752602, 'eval_runtime': 7.5306, 'eval_samples_per_second': 176.612, 'eval_steps_per_second': 22.176, 'epoch': 6.0}\n",
            " 60% 1014/1690 [04:18<02:11,  5.14it/s]\n",
            "100% 167/167 [00:07<00:00, 23.74it/s]\u001b[A\n",
            "{'loss': 0.0487, 'learning_rate': 6.666666666666667e-06, 'epoch': 7.0}\n",
            " 70% 1183/1690 [04:54<01:52,  4.52it/s]\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 4/167 [00:00<00:04, 38.08it/s]\u001b[A\n",
            "  5% 8/167 [00:00<00:05, 28.70it/s]\u001b[A\n",
            "  7% 12/167 [00:00<00:04, 31.65it/s]\u001b[A\n",
            " 10% 16/167 [00:00<00:05, 29.81it/s]\u001b[A\n",
            " 12% 20/167 [00:00<00:05, 28.14it/s]\u001b[A\n",
            " 14% 23/167 [00:00<00:05, 27.97it/s]\u001b[A\n",
            " 16% 26/167 [00:00<00:05, 27.63it/s]\u001b[A\n",
            " 18% 30/167 [00:01<00:04, 29.19it/s]\u001b[A\n",
            " 20% 34/167 [00:01<00:04, 30.64it/s]\u001b[A\n",
            " 23% 38/167 [00:01<00:04, 30.11it/s]\u001b[A\n",
            " 25% 42/167 [00:01<00:04, 29.49it/s]\u001b[A\n",
            " 28% 46/167 [00:01<00:03, 31.51it/s]\u001b[A\n",
            " 30% 50/167 [00:01<00:03, 31.68it/s]\u001b[A\n",
            " 32% 54/167 [00:01<00:03, 29.66it/s]\u001b[A\n",
            " 35% 58/167 [00:01<00:03, 29.90it/s]\u001b[A\n",
            " 37% 62/167 [00:02<00:03, 31.09it/s]\u001b[A\n",
            " 40% 66/167 [00:02<00:03, 30.63it/s]\u001b[A\n",
            " 42% 70/167 [00:02<00:03, 30.56it/s]\u001b[A\n",
            " 44% 74/167 [00:02<00:03, 28.90it/s]\u001b[A\n",
            " 47% 78/167 [00:02<00:03, 29.41it/s]\u001b[A\n",
            " 49% 82/167 [00:02<00:02, 30.86it/s]\u001b[A\n",
            " 51% 86/167 [00:02<00:02, 30.60it/s]\u001b[A\n",
            " 54% 90/167 [00:02<00:02, 31.82it/s]\u001b[A\n",
            " 56% 94/167 [00:03<00:02, 32.77it/s]\u001b[A\n",
            " 59% 98/167 [00:03<00:02, 32.44it/s]\u001b[A\n",
            " 62% 103/167 [00:03<00:01, 35.32it/s]\u001b[A\n",
            " 64% 107/167 [00:03<00:01, 33.83it/s]\u001b[A\n",
            " 67% 112/167 [00:03<00:01, 33.96it/s]\u001b[A\n",
            " 70% 117/167 [00:03<00:01, 36.49it/s]\u001b[A\n",
            " 72% 121/167 [00:03<00:01, 36.82it/s]\u001b[A\n",
            " 75% 125/167 [00:03<00:01, 36.98it/s]\u001b[A\n",
            " 78% 130/167 [00:04<00:00, 38.04it/s]\u001b[A\n",
            " 81% 135/167 [00:04<00:00, 39.85it/s]\u001b[A\n",
            " 84% 140/167 [00:04<00:00, 42.33it/s]\u001b[A\n",
            " 87% 145/167 [00:04<00:00, 41.62it/s]\u001b[A\n",
            " 90% 150/167 [00:04<00:00, 39.34it/s]\u001b[A\n",
            " 93% 155/167 [00:04<00:00, 40.62it/s]\u001b[A\n",
            " 96% 160/167 [00:04<00:00, 41.40it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9091516137123108, 'eval_accuracy': 0.8233082706766918, 'eval_precision': 0.822416221778313, 'eval_recall': 0.7920102138765462, 'eval_f1': 0.8053239185298572, 'eval_runtime': 5.0317, 'eval_samples_per_second': 264.327, 'eval_steps_per_second': 33.19, 'epoch': 7.0}\n",
            " 70% 1183/1690 [04:59<01:52,  4.52it/s]\n",
            "100% 167/167 [00:04<00:00, 40.79it/s]\u001b[A\n",
            "{'loss': 0.0325, 'learning_rate': 4.444444444444444e-06, 'epoch': 8.0}\n",
            " 80% 1352/1690 [05:33<01:13,  4.58it/s]\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 4/167 [00:00<00:04, 39.28it/s]\u001b[A\n",
            "  5% 8/167 [00:00<00:05, 29.21it/s]\u001b[A\n",
            "  8% 13/167 [00:00<00:04, 34.65it/s]\u001b[A\n",
            " 10% 17/167 [00:00<00:04, 30.25it/s]\u001b[A\n",
            " 13% 21/167 [00:00<00:04, 29.56it/s]\u001b[A\n",
            " 15% 25/167 [00:00<00:04, 29.96it/s]\u001b[A\n",
            " 17% 29/167 [00:00<00:04, 30.90it/s]\u001b[A\n",
            " 20% 33/167 [00:01<00:04, 31.03it/s]\u001b[A\n",
            " 22% 37/167 [00:01<00:04, 31.46it/s]\u001b[A\n",
            " 25% 41/167 [00:01<00:03, 32.92it/s]\u001b[A\n",
            " 27% 45/167 [00:01<00:03, 32.38it/s]\u001b[A\n",
            " 29% 49/167 [00:01<00:03, 34.29it/s]\u001b[A\n",
            " 32% 53/167 [00:01<00:03, 33.48it/s]\u001b[A\n",
            " 34% 57/167 [00:01<00:03, 30.78it/s]\u001b[A\n",
            " 37% 61/167 [00:01<00:03, 31.19it/s]\u001b[A\n",
            " 39% 65/167 [00:02<00:03, 31.06it/s]\u001b[A\n",
            " 41% 69/167 [00:02<00:03, 31.99it/s]\u001b[A\n",
            " 44% 73/167 [00:02<00:03, 30.36it/s]\u001b[A\n",
            " 46% 77/167 [00:02<00:02, 31.38it/s]\u001b[A\n",
            " 49% 81/167 [00:02<00:02, 32.50it/s]\u001b[A\n",
            " 51% 85/167 [00:02<00:02, 33.29it/s]\u001b[A\n",
            " 53% 89/167 [00:02<00:02, 33.09it/s]\u001b[A\n",
            " 56% 93/167 [00:02<00:02, 33.57it/s]\u001b[A\n",
            " 58% 97/167 [00:03<00:02, 32.48it/s]\u001b[A\n",
            " 61% 102/167 [00:03<00:01, 36.01it/s]\u001b[A\n",
            " 63% 106/167 [00:03<00:01, 35.03it/s]\u001b[A\n",
            " 66% 110/167 [00:03<00:01, 35.20it/s]\u001b[A\n",
            " 68% 114/167 [00:03<00:01, 35.41it/s]\u001b[A\n",
            " 71% 119/167 [00:03<00:01, 38.33it/s]\u001b[A\n",
            " 74% 123/167 [00:03<00:01, 36.46it/s]\u001b[A\n",
            " 77% 128/167 [00:03<00:01, 38.63it/s]\u001b[A\n",
            " 79% 132/167 [00:03<00:00, 38.83it/s]\u001b[A\n",
            " 82% 137/167 [00:04<00:00, 39.63it/s]\u001b[A\n",
            " 85% 142/167 [00:04<00:00, 41.25it/s]\u001b[A\n",
            " 88% 147/167 [00:04<00:00, 41.85it/s]\u001b[A\n",
            " 91% 152/167 [00:04<00:00, 40.40it/s]\u001b[A\n",
            " 94% 157/167 [00:04<00:00, 41.30it/s]\u001b[A\n",
            " 97% 162/167 [00:04<00:00, 42.14it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9974769949913025, 'eval_accuracy': 0.8097744360902256, 'eval_precision': 0.820639249501513, 'eval_recall': 0.7625316875694219, 'eval_f1': 0.784527766542184, 'eval_runtime': 4.8581, 'eval_samples_per_second': 273.771, 'eval_steps_per_second': 34.376, 'epoch': 8.0}\n",
            " 80% 1352/1690 [05:38<01:13,  4.58it/s]\n",
            "100% 167/167 [00:04<00:00, 41.31it/s]\u001b[A\n",
            "{'loss': 0.0184, 'learning_rate': 2.222222222222222e-06, 'epoch': 9.0}\n",
            " 90% 1521/1690 [06:19<00:39,  4.28it/s]\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 4/167 [00:00<00:04, 38.42it/s]\u001b[A\n",
            "  5% 8/167 [00:00<00:05, 27.03it/s]\u001b[A\n",
            "  7% 11/167 [00:00<00:05, 26.07it/s]\u001b[A\n",
            "  8% 14/167 [00:00<00:06, 24.77it/s]\u001b[A\n",
            " 10% 17/167 [00:00<00:06, 23.26it/s]\u001b[A\n",
            " 12% 20/167 [00:00<00:06, 23.95it/s]\u001b[A\n",
            " 14% 23/167 [00:00<00:05, 24.76it/s]\u001b[A\n",
            " 16% 26/167 [00:01<00:05, 24.25it/s]\u001b[A\n",
            " 17% 29/167 [00:01<00:05, 23.29it/s]\u001b[A\n",
            " 19% 32/167 [00:01<00:05, 23.48it/s]\u001b[A\n",
            " 22% 36/167 [00:01<00:05, 25.37it/s]\u001b[A\n",
            " 23% 39/167 [00:01<00:04, 26.16it/s]\u001b[A\n",
            " 25% 42/167 [00:01<00:04, 25.30it/s]\u001b[A\n",
            " 27% 45/167 [00:01<00:04, 26.02it/s]\u001b[A\n",
            " 29% 48/167 [00:01<00:04, 26.73it/s]\u001b[A\n",
            " 31% 51/167 [00:02<00:04, 26.72it/s]\u001b[A\n",
            " 32% 54/167 [00:02<00:04, 25.30it/s]\u001b[A\n",
            " 34% 57/167 [00:02<00:04, 24.85it/s]\u001b[A\n",
            " 36% 60/167 [00:02<00:04, 23.96it/s]\u001b[A\n",
            " 38% 63/167 [00:02<00:04, 23.39it/s]\u001b[A\n",
            " 40% 66/167 [00:02<00:04, 22.63it/s]\u001b[A\n",
            " 41% 69/167 [00:02<00:04, 23.91it/s]\u001b[A\n",
            " 43% 72/167 [00:02<00:04, 23.52it/s]\u001b[A\n",
            " 45% 75/167 [00:03<00:03, 24.11it/s]\u001b[A\n",
            " 47% 78/167 [00:03<00:03, 25.26it/s]\u001b[A\n",
            " 49% 82/167 [00:03<00:03, 26.93it/s]\u001b[A\n",
            " 51% 85/167 [00:03<00:02, 27.69it/s]\u001b[A\n",
            " 53% 88/167 [00:03<00:02, 27.21it/s]\u001b[A\n",
            " 54% 91/167 [00:03<00:02, 27.50it/s]\u001b[A\n",
            " 57% 95/167 [00:03<00:02, 28.29it/s]\u001b[A\n",
            " 59% 99/167 [00:03<00:02, 29.82it/s]\u001b[A\n",
            " 62% 103/167 [00:03<00:02, 31.00it/s]\u001b[A\n",
            " 64% 107/167 [00:04<00:01, 30.36it/s]\u001b[A\n",
            " 66% 111/167 [00:04<00:01, 31.32it/s]\u001b[A\n",
            " 69% 115/167 [00:04<00:01, 31.62it/s]\u001b[A\n",
            " 72% 120/167 [00:04<00:01, 33.29it/s]\u001b[A\n",
            " 74% 124/167 [00:04<00:01, 33.48it/s]\u001b[A\n",
            " 77% 128/167 [00:04<00:01, 33.44it/s]\u001b[A\n",
            " 79% 132/167 [00:04<00:01, 32.30it/s]\u001b[A\n",
            " 81% 136/167 [00:04<00:00, 33.00it/s]\u001b[A\n",
            " 84% 140/167 [00:05<00:00, 33.79it/s]\u001b[A\n",
            " 86% 144/167 [00:05<00:00, 34.77it/s]\u001b[A\n",
            " 89% 148/167 [00:05<00:00, 35.04it/s]\u001b[A\n",
            " 91% 152/167 [00:05<00:00, 34.63it/s]\u001b[A\n",
            " 94% 157/167 [00:05<00:00, 36.43it/s]\u001b[A\n",
            " 96% 161/167 [00:05<00:00, 37.26it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9872796535491943, 'eval_accuracy': 0.8150375939849624, 'eval_precision': 0.8151286840244225, 'eval_recall': 0.787434897099825, 'eval_f1': 0.7986177571925711, 'eval_runtime': 5.9231, 'eval_samples_per_second': 224.546, 'eval_steps_per_second': 28.195, 'epoch': 9.0}\n",
            " 90% 1521/1690 [06:25<00:39,  4.28it/s]\n",
            "100% 167/167 [00:05<00:00, 34.98it/s]\u001b[A\n",
            "{'loss': 0.0145, 'learning_rate': 0.0, 'epoch': 10.0}\n",
            "100% 1690/1690 [07:01<00:00,  5.45it/s]\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 4/167 [00:00<00:04, 37.55it/s]\u001b[A\n",
            "  5% 8/167 [00:00<00:05, 28.20it/s]\u001b[A\n",
            "  7% 12/167 [00:00<00:04, 31.85it/s]\u001b[A\n",
            " 10% 16/167 [00:00<00:05, 30.08it/s]\u001b[A\n",
            " 12% 20/167 [00:00<00:05, 28.32it/s]\u001b[A\n",
            " 14% 23/167 [00:00<00:05, 28.43it/s]\u001b[A\n",
            " 16% 27/167 [00:00<00:04, 29.77it/s]\u001b[A\n",
            " 19% 31/167 [00:01<00:04, 30.53it/s]\u001b[A\n",
            " 21% 35/167 [00:01<00:04, 30.53it/s]\u001b[A\n",
            " 23% 39/167 [00:01<00:04, 31.18it/s]\u001b[A\n",
            " 26% 43/167 [00:01<00:04, 30.06it/s]\u001b[A\n",
            " 29% 48/167 [00:01<00:03, 33.61it/s]\u001b[A\n",
            " 31% 52/167 [00:01<00:03, 33.03it/s]\u001b[A\n",
            " 34% 56/167 [00:01<00:03, 29.49it/s]\u001b[A\n",
            " 36% 60/167 [00:01<00:03, 31.00it/s]\u001b[A\n",
            " 38% 64/167 [00:02<00:03, 31.41it/s]\u001b[A\n",
            " 41% 68/167 [00:02<00:03, 32.28it/s]\u001b[A\n",
            " 43% 72/167 [00:02<00:03, 30.34it/s]\u001b[A\n",
            " 46% 76/167 [00:02<00:02, 31.64it/s]\u001b[A\n",
            " 48% 80/167 [00:02<00:02, 31.85it/s]\u001b[A\n",
            " 50% 84/167 [00:02<00:02, 33.23it/s]\u001b[A\n",
            " 53% 88/167 [00:02<00:02, 32.39it/s]\u001b[A\n",
            " 55% 92/167 [00:02<00:02, 32.21it/s]\u001b[A\n",
            " 57% 96/167 [00:03<00:02, 31.20it/s]\u001b[A\n",
            " 60% 101/167 [00:03<00:01, 34.52it/s]\u001b[A\n",
            " 63% 106/167 [00:03<00:01, 34.49it/s]\u001b[A\n",
            " 66% 110/167 [00:03<00:01, 34.51it/s]\u001b[A\n",
            " 68% 114/167 [00:03<00:01, 34.35it/s]\u001b[A\n",
            " 71% 119/167 [00:03<00:01, 36.49it/s]\u001b[A\n",
            " 74% 123/167 [00:03<00:01, 34.63it/s]\u001b[A\n",
            " 77% 128/167 [00:03<00:01, 37.01it/s]\u001b[A\n",
            " 79% 132/167 [00:04<00:00, 36.73it/s]\u001b[A\n",
            " 82% 137/167 [00:04<00:00, 38.19it/s]\u001b[A\n",
            " 84% 141/167 [00:04<00:00, 38.60it/s]\u001b[A\n",
            " 87% 145/167 [00:04<00:00, 38.96it/s]\u001b[A\n",
            " 89% 149/167 [00:04<00:00, 37.79it/s]\u001b[A\n",
            " 92% 153/167 [00:04<00:00, 36.83it/s]\u001b[A\n",
            " 95% 158/167 [00:04<00:00, 38.47it/s]\u001b[A\n",
            " 98% 163/167 [00:04<00:00, 38.89it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.995873749256134, 'eval_accuracy': 0.818796992481203, 'eval_precision': 0.8191093383294609, 'eval_recall': 0.7895269985019584, 'eval_f1': 0.8013003011183727, 'eval_runtime': 5.0832, 'eval_samples_per_second': 261.645, 'eval_steps_per_second': 32.853, 'epoch': 10.0}\n",
            "100% 1690/1690 [07:06<00:00,  5.45it/s]\n",
            "100% 167/167 [00:05<00:00, 37.73it/s]\u001b[A\n",
            "{'train_runtime': 426.2672, 'train_samples_per_second': 63.341, 'train_steps_per_second': 3.965, 'train_loss': 0.23940224055002426, 'epoch': 10.0}\n",
            "100% 1690/1690 [07:06<00:00,  3.96it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     0.2394\n",
            "  train_runtime            = 0:07:06.26\n",
            "  train_samples            =       2700\n",
            "  train_samples_per_second =     63.341\n",
            "  train_steps_per_second   =      3.965\n",
            "100% 167/167 [00:04<00:00, 34.43it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       10.0\n",
            "  eval_accuracy           =     0.8188\n",
            "  eval_f1                 =     0.8013\n",
            "  eval_loss               =     0.9959\n",
            "  eval_precision          =     0.8191\n",
            "  eval_recall             =     0.7895\n",
            "  eval_runtime            = 0:00:04.90\n",
            "  eval_samples            =       1330\n",
            "  eval_samples_per_second =    271.115\n",
            "  eval_steps_per_second   =     34.042\n",
            " 99% 249/252 [00:08<00:00, 26.25it/s]/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100% 252/252 [00:08<00:00, 28.57it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.5605\n",
            "  predict_f1                 =     0.2395\n",
            "  predict_loss               =     2.7297\n",
            "  predict_precision          =     0.3333\n",
            "  predict_recall             =     0.1868\n",
            "  predict_runtime            = 0:00:08.85\n",
            "  predict_samples_per_second =    227.781\n",
            "  predict_steps_per_second   =     28.473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/models/banglabert/predictions.txt', 'r') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Process the lines to create a list of lists for CSV data\n",
        "csv_data = []\n",
        "for line in lines[1:]:  # Skip the header line\n",
        "    index, prediction = line.strip().split('\\t')\n",
        "    csv_data.append([index, prediction])\n",
        "\n",
        "# Write the CSV data to a new CSV file\n",
        "with open('/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/models/banglabert/predictions.csv', 'w', newline='') as outfile:\n",
        "    csv_writer = csv.writer(outfile)\n",
        "    csv_writer.writerow(['index', 'prediction'])  # Write the header\n",
        "    csv_writer.writerows(csv_data)\n"
      ],
      "metadata": {
        "id": "pWx6uEzlhFie"
      },
      "execution_count": 26,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}